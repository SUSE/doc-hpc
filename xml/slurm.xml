<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-slurm" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Slurm</title>
 <info>
  <abstract>
   <para>
     <emphasis>Slurm</emphasis> is a workload manager for managing compute jobs on
       high performance computing cluster. It can start multiple jobs on a single
       node or a single job on multiple nodes. Additional components can be used for 
       advanced scheduling and accounting.
   </para>
   <para>
      The mandatory components of <emphasis>Slurm</emphasis> is the controll daemon 
      <emphasis>slurmctld</emphasis> taking care of job scheduling and the slurm 
      daemon <emphasis>slurmd</emphasis> reposible for launching compute jobs. Subsequently
      nodes running the controll daemon are called master nodes and nodes running the
      <emphasis>slurmd</emphasis> are called compute nodes.
   </para>
      <para>
        Additional components are a secondary <emphasis>slurmctld</emphasis> acting 
        as standby server for a failover and the slurm database daemon 
        <emphasis>slurmdbd</emphasis> which stores the job history and user hirachy.
      </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-scheduler-slurm">
   <title>Slurm &mdash; Utility for HPC Workload Management</title>
   <para>
     For a minimal setup of <emphasis>Slurm</emphasis>
     with a controll node and multiple cmpute nodes, follow these instructions:
       </para>
         <sect2 xml:id="sec-slurm-minimal">
           <title>Minimal installation</title>
     <important>
      <title>Make Sure of Consistent UIDs and GIDs for Slurm's Accounts</title>
      <para>
        For security reasons, <emphasis>Slurm</emphasis> does not run as the user
       <systemitem class="username">root</systemitem> but under its own
       user. It is important that the user
       <systemitem class="username">slurm</systemitem> has the
       same UID/GID across all nodes of the cluster.
      </para>
      <para>
       If this user/group does not exist, the package
       <package>slurm</package> creates this user and group when it is
       installed. However, this does not guarantee
       that the generated UIDs/GIDs will be identical on all systems.
      </para>
      <para>
       Therefore, we strongly advise you to create the user/group
       <systemitem class="username">slurm</systemitem> before
       installing <package>slurm</package>.
       If you are using a network directory service such as LDAP for user and
       group management, you can use it to
       provide the <systemitem class="username">slurm</systemitem>
       user/group as well.
      </para>
     </important>
   <procedure>
    <step>
     <para>
      Install <package>slurm-node</package> on the compute
        nodes with <command>zypper in slurm-node</command> and <package>slurm</package> on 
          the controll node with <command>zypper in slurm</command>. The package <package>munge</package>will be installed as dependence automatically.
     </para>
    </step>
    <step>
     <para>
       Configure, enable and start <emphasis>munge</emphasis> on the control and compute nodes.
      as described in <xref linkend="sec-remote-munge"/>.
     </para>
    </step>
    <step>
     <title>Create <filename>slurm.conf</filename></title>
     <para>
      Edit the main configuration file <filename>/etc/slurm/slurm.conf</filename>:
     </para>
     <substeps>
      <step>
       <para>
        Configure the parameter
        <literal>ControlMachine=<replaceable>CONTROL_MACHINE</replaceable></literal>
        with the host name of the control node.
       </para>
       <para>
        To find out the correct host name, run
        <command>hostname -s</command> on the control node.
       </para>
      </step>
      <step>
       <para>
        In order to define the compute nodes add:
       </para>
<screen>NodeName=<replaceable>NODE_LIST</replaceable>  State=UNKNOWN</screen>
    <para>
      where the actual parameters like <literal>CPUs</literal> for compute node can be obtained by running 
<screen>slurmd -C</screen>
  on the compute node. 
  </para>
       <para>
       Additionally the line 
       </para>
<screen>PartitionName=normal Nodes=<replaceable>NODE_LIST</replaceable> \
  Default=YES MaxTime=24:00:00 State=UP</screen>
       <para>
        has to be added, where <replaceable>NODE_LIST</replaceable> is the list of compute
        nodes (that is, the output of <command>hostname -s</command> run on
        each compute node (either comma-separated or as ranges:
          <literal>node[1-100]</literal>). 
        </para>
      </step>
      <step>
       <para>
        Now copy the modified configuration file <filename>/etc/slurm/slurm.conf</filename>
        to the control node and all compute nodes:
       </para>
<screen>scp /etc/slurm/slurm.conf <replaceable>COMPUTE_NODE</replaceable>:/etc/slurm/</screen>
      </step>
     </substeps>
      </step>
      <step>
       <para>
        On the control node, start <systemitem class="daemon">slurmctld</systemitem>:
       </para>
<screen>systemctl start slurmctld.service</screen>
       <para>
        Also enable it so that it starts on every boot:
       </para>
<screen>systemctl enable slurmctld.service</screen>
      </step>
      <step>
       <para>
        On the compute nodes, start and enable
        <systemitem class="daemon">slurmd</systemitem>:
       </para>
<screen>systemctl start slurmd.service
systemctl enable slurmd.service</screen>
       <para>
        The last line causes <systemitem class="daemon">slurmd</systemitem>
        to be started on
        every boot automatically.
       </para>
    </step>
      <step>
        <title>Test installation</title>
          <substeps>
            <step>
              <title>Check node status</title>
                <para>
                  The status and availability of the compute nodes can be tested with the comand
                  <command>sinfo</command>
                    and the output should give you in this example
<screen>
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST 
normal*      up 1-00:00:00      2   idle sle15sp2-node[1-2] 
</screen>
If the node state is not <literal>idle</literal> see <xref linkend="sec-slurm-faq"/>.
                  </para>
            </step>
          </substeps>
        </step>
   </procedure>
 </sect2>
   <sect2 xml:id="sec-slurm-test">
    <title>Testing slurm installation</title>
      <para>
          For test
        </para>
     </sect2>
   <!--
   <note>
    <title>Epilog Script</title>
    <para>
     The standard epilog script will kill all remaining processes of a user
     on a node. If this behavior is not wanted, disable the standard epilog
     script.
    </para>
   </note>
     -->
     </sect1>
    <sect1  xml:id="sec-slurm-commands">
      <title>Slurm commands</title>
      <para>
        To be added
      </para>
    </sect1>
     <sect1 xml:id="sec-slurm-additional-res">
       <title>Additional Ressources</title>
     <sect2 xml:id="sec-slurm-faq">
       <title>Frequently asked questions</title>
       <para>
          Faq
        </para>
     </sect2>
     <sect2 xml:id="sec-slurm-ext-doc">
       <title>External documentaion</title>
   <para>
    For further documentation, see the
    <link xlink:href="https://slurm.schedmd.com/quickstart_admin.html">Quick Start
        Administrator Guide</link> and
<link xlink:href="https://slurm.schedmd.com/quickstart.html"> Quick Start User
    Guide</link>. There is further in-depth documentation on the
<link xlink:href="https://slurm.schedmd.com/documentation.html">Slurm
    documentation page</link>.
      </para>
    </sect2>
        </sect1>
 <!--
 <sect1 xml:id="sec-scheduler-conf">
  <title>Configuration</title>
  <para>
   TOFIX
  </para>
  <orderedlist>
   <listitem>
    <para>
     An
    </para>
   </listitem>
   <listitem>
    <para>
     Ordered
    </para>
   </listitem>
   <listitem>
    <para>
     List
    </para>
   </listitem>
  </orderedlist>
 </sect1>
  -->
</chapter>
