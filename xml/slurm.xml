<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-slurm" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Slurm</title>
 <info>
  <abstract>
   <para>
     <emphasis>Slurm</emphasis> is a workload manager for managing compute jobs on
       high performance computing cluster. It can start multiple jobs on a single
       node or a single job on multiple nodes. Additional components can be used for 
       advanced scheduling and accounting.
   </para>
   <para>
      The mandatory components of <emphasis>Slurm</emphasis> is the control daemon 
      <emphasis>slurmctld</emphasis> taking care of job scheduling and the slurm 
      daemon <emphasis>slurmd</emphasis> responsible for launching compute jobs. Subsequently
      nodes running the control daemon are called master nodes and nodes running the
      <emphasis>slurmd</emphasis> are called compute nodes.
   </para>
      <para>
        Additional components are a secondary <emphasis>slurmctld</emphasis> acting 
        as standby server for a failover and the slurm database daemon 
        <emphasis>slurmdbd</emphasis> which stores the job history and user hirachy.
      </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-scheduler-slurm">
   <title>Slurm &mdash; Utility for HPC Workload Management</title>
   <para>
     For a minimal setup of <emphasis>Slurm</emphasis>
     with a controll node and multiple cmpute nodes, follow these instructions:
       </para>
         <sect2 xml:id="sec-slurm-minimal">
           <title>Minimal installation</title>
     <important>
      <title>Make Sure of Consistent UIDs and GIDs for Slurm's Accounts</title>
      <para>
        For security reasons, <emphasis>Slurm</emphasis> does not run as the user
       <systemitem class="username">root</systemitem> but under its own
       user. It is important that the user
       <systemitem class="username">slurm</systemitem> has the
       same UID/GID across all nodes of the cluster.
      </para>
      <para>
       If this user/group does not exist, the package
       <package>slurm</package> creates this user and group when it is
       installed. However, this does not guarantee
       that the generated UIDs/GIDs will be identical on all systems.
      </para>
      <para>
       Therefore, we strongly advise you to create the user/group
       <systemitem class="username">slurm</systemitem> before
       installing <package>slurm</package>.
       If you are using a network directory service such as LDAP for user and
       group management, you can use it to
       provide the <systemitem class="username">slurm</systemitem>
       user/group as well.
      </para>
     </important>
   <procedure>
    <step>
     <para>
      Install <package>slurm-node</package> on the compute
        nodes with <command>zypper in slurm-node</command> and <package>slurm</package> on 
          the controll node with <command>zypper in slurm</command>. The package <package>munge</package>will be installed as dependence automatically.
     </para>
    </step>
    <step>
     <para>
       Configure, enable and start <emphasis>munge</emphasis> on the control and compute nodes.
      as described in <xref linkend="sec-remote-munge"/>.
     </para>
    </step>
    <step>
     <title>Create <filename>slurm.conf</filename></title>
     <para>
      Edit the main configuration file <filename>/etc/slurm/slurm.conf</filename>:
     </para>
     <substeps>
      <step>
       <para>
        Configure the parameter
        <literal>ControlMachine=<replaceable>CONTROL_MACHINE</replaceable></literal>
        with the host name of the control node.
       </para>
       <para>
        To find out the correct host name, run
        <command>hostname -s</command> on the control node.
       </para>
      </step>
      <step>
       <para>
        In order to define the compute nodes add:
       </para>
<screen>NodeName=<replaceable>NODE_LIST</replaceable>  State=UNKNOWN</screen>
    <para>
      where the actual parameters like <literal>CPUs</literal> for compute node can be obtained by running 
<screen>slurmd -C</screen>
  on the compute node. 
  </para>
       <para>
       Additionally the line 
       </para>
<screen>PartitionName=normal Nodes=<replaceable>NODE_LIST</replaceable> \
  Default=YES MaxTime=24:00:00 State=UP</screen>
       <para>
        has to be added, where <replaceable>NODE_LIST</replaceable> is the list of compute
        nodes (that is, the output of <command>hostname -s</command> run on
        each compute node (either comma-separated or as ranges:
          <literal>node[1-100]</literal>). 
        </para>
      </step>
      <step>
       <para>
        Now copy the modified configuration file <filename>/etc/slurm/slurm.conf</filename>
        to the control node and all compute nodes:
       </para>
<screen>scp /etc/slurm/slurm.conf <replaceable>COMPUTE_NODE</replaceable>:/etc/slurm/</screen>
      </step>
     </substeps>
      </step>
      <step>
       <para>
        On the control node, start <systemitem class="daemon">slurmctld</systemitem>:
       </para>
<screen>systemctl start slurmctld.service</screen>
       <para>
        Also enable it so that it starts on every boot:
       </para>
<screen>systemctl enable slurmctld.service</screen>
      </step>
      <step>
       <para>
        On the compute nodes, start and enable
        <systemitem class="daemon">slurmd</systemitem>:
       </para>
<screen>systemctl start slurmd.service
systemctl enable slurmd.service</screen>
       <para>
        The last line causes <systemitem class="daemon">slurmd</systemitem>
        to be started on
        every boot automatically.
       </para>
    </step>
      <step>
        <title>Test installation</title>
          <substeps>
            <step>
              <title>Check node status</title>
                <para>
                  The status and availability of the compute nodes can be tested with the comand
                  <command>sinfo</command>
                    and the output should give you in this example
<screen>
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST 
normal*      up 1-00:00:00      2   idle sle15sp2-node[1-2] 
</screen>
If the node state is not <literal>idle</literal> see <xref linkend="sec-slurm-faq"/>.
                  </para>
            </step>
            <step>
              <title>Run command</title>
                <para>
                  Now the slurm installation can be tested by running
<screen>srun sleep 30</screen>
  which will try to immediatelly run the  <command>sleep</command> on a free compute node. In another shell you can now <footnote><para>at least within 30 seconds</para></footnote>run the command 
    <screen>squeue</screen> 
      which will show you the running compute job in an output like
<screen>
    JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) 
        1    normal    sleep     root  R       0:05      1 node02 
</screen>
                  </para>
            </step>
            <step>
              <title>Run shell script</title>
                <para>
                Now you can create the simple shell script
<screen>
#!/bin/bash
echo "started at $(date)"
sleep 30
echo "finished at $(date)"
</screen>
  save it as <filename>sleeper.sh</filename> and run the shell script in the queue with 
    <screen>
sbatch sleeper.sh
      </screen>
        so that the shell script is executed as soon as enough resources are available. After the execution the output of the shell script is stored in the output file <filename>slurm-${JOBNR}.out</filename>.
          </para>
            </step>

          </substeps>
        </step>
   </procedure>
 </sect2>
   <sect2 xml:id="sec-slurm-slurmdbd">
    <title>Install slurm database</title>
      <para>
       With the minimal installation slurm will only store pending and running jobs. In order to store to finished and failed job data, the storage plugin has to be installed and enabled. Additionally so called completely fair scheduling can be enable, which replaces the FIFO<footnote><para>><emphasis>f</emphasis>irst <emphasis>i</emphasis>n <emphasis>f</emphasis>irst <emphasis>o</emphasis>ut.</para></footnote> scheduling with algorithms which calculate the job priority in a queue in dependence of the job which a user has run in the history.
         </para>
           <para>
             The slurm database has two components which is the daemon <literal>slurmdbd</literal> itself and a mysql database, where <literal>mariab</literal> is recommended. The database can be on the same node which runs <literal>slurmdbd</literal> or another node. For this simple setup all these services run on the same node.
            </para>
              <procedure>
                <title>Install <package>slurmdbd</package></title>
                  <step>
                    <title>Install mariadb</title>
                      <para>
                        If a external SQL database should be used or such database is already installed on the control node, this step can be skipped.
                      </para>
                      <para>
                            The mariabd SQL database can be installed with the command
                            <screen>zypper in mariadb</screen>
                            after the installation the database should be started and enabled with commands
                            <screen>
systemctl start mariadb
systemctl enable mariadb
                            </screen>
                              Now the database should be secure with the command <command>mysql_secure_installation</command>.
                        </para>
                    </step>
                    <step xml:id="sec-sum-sqldb"><title>Create database for slurm</title>
                      <para>
                        In this step the database and the user for the slurm database has to be created. This is done by connecting to the SQL database with e.g. the command <command>mysql -u root -p</command>. After a successful connection the database and the creation of secure password<footnote><para>You can use the command <command>openssl rand -base64 32</command> to create a secure random password</para></footnote>, the slurm user and the database is created with the commands
                          <screen>
create user 'slurm'@'localhost' identified by 'password';
grant all on slurm_acct_db.* TO 'slurm'@'localhost';
create database slurm_acct_db;
                            </screen>
                      </para>
                    </step>
                      <step><title>Install <literal>slurmdbd</literal></title>
                        <para>
                          The package <package>slurmdbd</package> can be installed with the command
                        <screen>
zypper in slurm-slurmdbd
                        </screen>
                          Now the configuration file <filename>/etc/slurm/slurmdbd.conf</filename> for <literal>slurmdbd</literal> has to be modified so that the daemon can access the database. Change the following lines
<screen>
StoragePass=password
  </screen>
    to the password which you use in <xref linkend="sec-sum-sqldb"/>. When another location or user for the SQL database was chosen, also the entries for 
<screen>
StorageUser=slurm
DbdAddr=localhost
DbdHost=localhost
</screen>
  have to be modified.
                        </para>
                    </step>
                    <step> <title>Start and enable <literal>slurmdbd</literal></title>
                      <para>
                        The daemon <literal>slurmdbd</literal> should now be started and enabled with the commands
<screen>
systemctl start slurmdbd
systemctl enable slurmdbd
</screen>
  , please note that the first start of <literal>slurmdbd</literal> will take some time.
    </para>
                    </step>
                      <step><title>Enable accounting</title>
                        <para>
                          For the connection between the <literal>slurmctld</literal> and the <literal>slurmdbd</literal> daemon you have change/add following lines 
                            <screen>
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost
                            </screen>
                              assuming that <literal>slurmdbd</literal> is running on the same host as <literal>slurmctld</literal>. After this step restart the <literal>slurmctld</literal> with
                                <screen>
systemctl restart slurmctld
                               </screen>
                        </para>
                    </step>
                    <step performance="optional">
                      <title>Enable accounring per group</title>
                        <para>
                          Per default <literal>slurm</literal> does not take any group membership into account and it is not possible to map the system groups to <literal>slurm</literal>. The creation and group membership have to managed via the commandline tool <command>sacctmgr</command>. Its also possible to have a group hierarchy and users can be part of several groups. 
                        </para>
                        <para>
                          To create an umbrella group <literal>bavaria</literal> for two subgroups called <literal>nuremberg</literal> and <literal>munich</literal> use following commads
<screen>
sacctmgr add account bavaria \
  Description="umbrella group for subgroups" Organization=bavaria
sacctmgr add account nuremberg,munich parent=bavaria 
  Description="subgroup" Organization=bavaria
</screen>
                          With the similar syntax you can add now users to the accounts
<screen>
sacctmgr add user tux Account=nuremberg
</screen>
                        </para>
                    </step>
                </procedure>
     </sect2>
   <!--
   <note>
    <title>Epilog Script</title>
    <para>
     The standard epilog script will kill all remaining processes of a user
     on a node. If this behavior is not wanted, disable the standard epilog
     script.
    </para>
   </note>
     -->
     </sect1>
     <sect1 xml:id="sec-slurm-adm-commands">
     <title>Slurm administration commands</title>
     <sect2 xml:id="sec-slurm-sconfigure">
       <title>scontrol</title>
       <para>
         The command <command>scontrol</command> is used to show and update the entities of <literal>slurm</literal> like the state of the compute nodes or compute jobs. It can also be used to reboot or propagate configuration changes to the compute nodes.
       </para>
       <variablelist>
         <varlistentry>
             <term><command>scontrol show <replaceable>ENTITY</replaceable></command></term>
            <para>
              will display the state of the specified <replaceable>ENTITY</replaceable>. 
            </para>
         </varlistentry>

        </variablelist>
      </sect2>
    </sect1>

     <sect1 xml:id="sec-slurm-additional-res">
       <title>Additional Ressources</title>
     <sect2 xml:id="sec-slurm-faq">
       <title>Frequently asked questions</title>
       <para>
          Faq
        </para>
     </sect2>
     <sect2 xml:id="sec-slurm-ext-doc">
       <title>External documentaion</title>
   <para>
    For further documentation, see the
    <link xlink:href="https://slurm.schedmd.com/quickstart_admin.html">Quick Start
        Administrator Guide</link> and
<link xlink:href="https://slurm.schedmd.com/quickstart.html"> Quick Start User
    Guide</link>. There is further in-depth documentation on the
<link xlink:href="https://slurm.schedmd.com/documentation.html">Slurm
    documentation page</link>.
      </para>
    </sect2>
        </sect1>
 <!--
 <sect1 xml:id="sec-scheduler-conf">
  <title>Configuration</title>
  <para>
   TOFIX
  </para>
  <orderedlist>
   <listitem>
    <para>
     An
    </para>
   </listitem>
   <listitem>
    <para>
     Ordered
    </para>
   </listitem>
   <listitem>
    <para>
     List
    </para>
   </listitem>
  </orderedlist>
 </sect1>
  -->
</chapter>
