<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="slurm.xml"
 xml:id="cha-slurm" xml:lang="en" version="5.1">
 <title>&slurm; â€” utility for &hpca; workload management</title>
 <info>
  <abstract>
   <para>
    <emphasis>&slurm;</emphasis> is a workload manager for managing compute jobs
    on &hpc; clusters. It can start multiple jobs on a
    single node, or a single job on multiple nodes. Additional components can be
    used for advanced scheduling and accounting.
   </para>

   <para>
    The mandatory components of &slurm; are the control daemon
    <emphasis>slurmctld</emphasis>, which handles job scheduling, and the
    &slurm; daemon <emphasis>slurmd</emphasis>, responsible for launching compute
    jobs. Nodes running <command>slurmctld</command> are called
    <emphasis>management servers</emphasis> and nodes running
    <command>slurmd</command> are called <emphasis>compute nodes</emphasis>.
   </para>

   <para>
    Additional components are a secondary <emphasis>slurmctld</emphasis> acting
    as a standby server for a failover, and the &slurm; database daemon
    <emphasis>slurmdbd</emphasis>, which stores the job history and user
    hierarchy.
   </para>

   <para>
    For further documentation, see the
    <link
     xlink:href="https://slurm.schedmd.com/quickstart_admin.html">Quick
    Start Administrator Guide</link> and
    <link
     xlink:href="https://slurm.schedmd.com/quickstart.html"> Quick
    Start User Guide</link>. There is further in-depth documentation on the
    <link
     xlink:href="https://slurm.schedmd.com/documentation.html">&slurm;
    documentation page</link>.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-scheduler-slurm">
  <title>Installing &slurm;</title>
  <para>
   These instructions describe a minimal installation of &slurm; with one
   management server and multiple compute nodes.
  </para>
  <sect2 xml:id="sec-slurm-minimal">
   <title>Minimal installation</title>
   <important>
    <title>Make sure of consistent UIDs and GIDs for &slurm;'s accounts</title>
    <para>
     For security reasons, &slurm; does not run as the user
     <systemitem
      class="username">root</systemitem>, but under its own
     user. It is important that the user
     <systemitem class="username">slurm</systemitem> has the same UID/GID
     across all nodes of the cluster.
    </para>
    <para>
     If this user/group does not exist, the package <package>slurm</package>
     creates this user and group when it is installed. However, this does not
     guarantee that the generated UIDs/GIDs will be identical on all systems.
    </para>
    <para>
     Therefore, we strongly advise you to create the user/group
     <systemitem class="username">slurm</systemitem> before installing
     <package>slurm</package>. If you are using a network directory service
     such as LDAP for user and group management, you can use it to provide the
     <systemitem class="username">slurm</systemitem> user/group as well.
    </para>
    <para>
     It is strongly recommended that all compute nodes share common user
     home directories. These should be provided through network storage.
    </para>
   </important>
   <procedure xml:id="pro-installing-slurm">
    <title>Installing the &slurm; packages</title>
    <step>
     <para>
      On the management server, install the <package>slurm</package> package with the
      command <command>zypper in slurm</command>.
     </para>
    </step>
    <step>
     <para>
      On the compute nodes, install the <package>slurm-node</package> package
      with the command <command>zypper in slurm-node</command>.
     </para>
    </step>
    <step>
     <para>
      On the management server and the compute nodes, the package
      <package>munge</package> is installed automatically. Configure, enable
      and start &munge; on the management server and compute nodes as
      described in <xref linkend="sec-remote-munge"/>. Ensure that the same
      <literal>munge</literal> key is shared across all nodes.
     </para>
    </step>
   </procedure>
   <note>
    <title>Automatically opened ports</title>
    <para>
     Installing the <package>slurm</package> package automatically opens the TCP
     ports 6817, 6818, and 6819.
    </para>
   </note>
   <procedure xml:id="pro-configuring-slurm">
    <title>Configuring &slurm;</title>
    <step>
     <para>
      On the management server, edit the main configuration file
      <filename>/etc/slurm/slurm.conf</filename>:
     </para>
     <substeps>
      <step>
       <para>
        Configure the parameter
        <literal>SlurmctldHost=<replaceable>SLURMCTLD_HOST</replaceable></literal>
        with the host name of the management server.
       </para>
       <para>
        To find the correct host name, run <command>hostname -s</command>
        on the management server.
       </para>
      </step>
      <step>
       <para>
        Under the <literal>COMPUTE NODES</literal> section, add the following
        lines to define the compute nodes:
       </para>
<screen>NodeName=<replaceable>NODE_LIST</replaceable> State=UNKNOWN
PartitionName=normal Nodes=<replaceable>NODE_LIST</replaceable> Default=YES MaxTime=24:00:00 State=UP</screen>
       <para>
        Replace <replaceable>NODE_LIST</replaceable> with the host names
        of the compute nodes, either comma-separated or as a range (for example:
        <literal>node[1-100]</literal>).
       </para>
       <para>
        The <literal>NodeName</literal> line also allows specifying additional
        parameters for the nodes, such as
        <literal>Boards</literal>, <literal>SocketsPerBoard</literal>
        <literal>CoresPerSocket</literal>, <literal>ThreadsPerCore</literal>,
        or <literal>CPU</literal>. The actual values of these can be
        obtained by running the following command on the compute nodes:
       </para>
<screen>&prompt.node1;slurmd -C</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Copy the modified configuration file
      <filename>/etc/slurm/slurm.conf</filename> from the management server to all
      compute nodes:
     </para>
<screen>&prompt.mgmt;scp /etc/slurm/slurm.conf <replaceable>COMPUTE_NODE</replaceable>:/etc/slurm/</screen>
    </step>
    <step>
     <para>
      On the management server, start <systemitem class="daemon">slurmctld</systemitem>
      and enable it so that it starts on every boot:
     </para>
<screen>&prompt.mgmt;systemctl enable --now slurmctld.service</screen>
    </step>
    <step>
     <para>
      On each compute node, start <systemitem class="daemon">slurmd</systemitem>
      and enable it so that it starts on every boot:
     </para>
<screen>&prompt.node1;systemctl enable --now slurmd.service</screen>
    </step>
   </procedure>
   <procedure xml:id="pro-testing-slurm-installation">
    <title>Testing the &slurm; installation</title>
    <step>
     <para>
      Check the status and availability of the compute nodes by running the
      <command>sinfo</command> command. You should see output similar to
      the following:
     </para>
<screen>
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      2   idle node[01-02]
</screen>
     <para>
      If the node state is not <literal>idle</literal>, see
      <xref
       linkend="sec-slurm-faq"/>.
     </para>
    </step>
    <step>
     <para>
      Test the &slurm; installation by running the following command:
     </para>
<screen>&prompt.mgmt;srun sleep 30</screen>
     <para>
      This runs the <command>sleep</command> command on a free compute node
      for 30 seconds.
     </para>
     <para>
      In another shell, run the <command>squeue</command> command during the
      30 seconds that the compute node is asleep. You should see output similar
      to the following:
     </para>
<screen>
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
    1    normal    sleep     root  R       0:05      1 node02
</screen>
    </step>
    <step>
     <para>
      Create the following shell script and save it as
      <filename>sleeper.sh</filename>:
     </para>
<screen>
#!/bin/bash
echo "started at $(date)"
sleep 30
echo "finished at $(date)"
</screen>
     <para>
      Run the shell script in the queue:
     </para>
<screen>&prompt.mgmt;sbatch sleeper.sh</screen>
     <para>
      The shell script is executed when enough resources are available,
      and the output is stored in the file <filename>slurm-${JOBNR}.out</filename>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-slurm-slurmdbd">
   <title>Installing the &slurm; database</title>
   <para>
    In a minimal installation, &slurm; only stores pending and running
    jobs. To store finished and failed job data, the storage plugin
    must be installed and enabled. You can also enable
    <emphasis>completely fair scheduling</emphasis>, which replaces FIFO
    (first in, first out) scheduling with algorithms that calculate the job
    priority in a queue in dependence of the job which a user has run in the history.
    <!-- FIX ME: I'm not quite sure what this last line is trying to say? -->
   </para>
   <para>
    The &slurm; database has two components: the <literal>slurmdbd</literal>
    daemon itself, and an SQL database. &mariadb; is recommended. The
    database can be installed on the same node that runs <literal>slurmdbd</literal>,
    or on a separate node. For a minimal setup, all these services run on the
    management server.
   </para>
   <procedure>
    <title>Install <package>slurmdbd</package></title>
    <note>
     <title>&mariadb;</title>
     <para>
      If you want to use an external SQL database (or you already have a
      database installed on the management server), you can skip
      <xref linkend="st-install-mariadb"/> and <xref linkend="st-start-mariadb"/>.
     </para>
    </note>
    <step xml:id="st-install-mariadb">
     <para>
      Install the &mariadb; SQL database with <command>zypper in
      mariadb</command>.
     </para>
    </step>
    <step xml:id="st-start-mariadb">
     <para>
      Start and enable &mariadb;:
     </para>
<screen>&prompt.mgmt;systemctl enable --now mariadb</screen>
    </step>
    <step>
     <para>
      Secure the database with the command
      <command>mysql_secure_installation</command>.
     </para>
    </step>
    <step>
     <para>
      Connect to the SQL database, for example with the command
      <command>mysql -u root -p</command>.
     </para>
    </step>
    <step xml:id="sec-sum-sqldb">
     <para>
      Create the &slurm; user and the database with the following commands:
     </para>
<screen>
&prompt.db;create user 'slurm'@'localhost' identified by '<replaceable>PASSWORD</replaceable>';
&prompt.db;grant all on slurm_acct_db.* TO 'slurm'@'localhost';
&prompt.db;create database slurm_acct_db;</screen>
     <para>
      After these steps are complete, exit the database.
     </para>
    </step>
    <step>
     <para>
      Install the <package>slurmdbd</package> package:
     </para>
<screen>&prompt.mgmt;zypper in slurm-slurmdbd</screen>
    </step>
    <step>
     <para>
      Edit the <filename>/etc/slurm/slurmdbd.conf</filename> file so that the
      daemon can access the database. Change the following line to the password
      that you used in <xref linkend="sec-sum-sqldb"/>:
     </para>
<screen>StoragePass=password</screen>
<!-- FIME: It doesn't seem like a good idea to direct people to store a plain text
     password. Maybe this should recommend a passfile instead? -->
     <para>
       If you chose another location or user for the SQL database, you must also
      modify the following lines:
     </para>
<screen>StorageUser=slurm
DbdAddr=localhost
DbdHost=localhost</screen>
    </step>
    <step>
     <para>
      Start and enable <literal>slurmdbd</literal>:
     </para>
<screen>&prompt.mgmt;systemctl enable --now slurmdbd</screen>
     <para>
      The first start of <literal>slurmdbd</literal> will take some time.
     </para>
    </step>
    <step>
     <para>
      To enable accounting, edit the <filename>/etc/slurm/slurm.conf</filename>
      file to add the connection between <literal>slurmctld</literal> and the
      <literal>slurmdbd</literal> daemon. Ensure that the following lines
      appear as shown:
     </para>
<screen>JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost</screen>
     <note role="compact">
      <para>
       This example assumes that <literal>slurmdbd</literal> is
      running on the same node as <literal>slurmctld</literal>. If not, change
      <literal>localhost</literal> to the host name or IP address of the node
      where <literal>slurmdbd</literal> is running.
      </para>
     </note>
    </step>
    <step>
     <para>
      Ensure that a table for the cluster is added to the database.
      Otherwise, no accounting information can be written to the database.
      To add a cluster table, run
      <command>sacctmgr -i add cluster <replaceable>CLUSTERNAME</replaceable></command>.
     </para>
    </step>
    <step>
     <para>
      Restart <literal>slurmctld</literal>:
     </para>
<screen>&prompt.mgmt;systemctl restart slurmctld</screen>
    </step>
    <step performance="optional">
     <para>
      By default, &slurm; does not take any group membership into account, and
      the system groups cannot be mapped to &slurm;. Group creation and
      membership must be managed via the command line tool
      <command>sacctmgr</command>. You can have a group
      hierarchy, and users can be part of several groups.
     </para>
     <para>
      The following example creates an umbrella group <literal>bavaria</literal>
      for two subgroups called <literal>nuremberg</literal> and <literal>munich</literal>:
     </para>
<screen>&prompt.mgmt;sacctmgr add account bavaria \
Description="umbrella group for subgroups" Organization=bavaria</screen>
<screen>&prompt.mgmt;sacctmgr add account nuremberg,munich parent=bavaria \
Description="subgroup" Organization=bavaria</screen>
     <para>
      The following example adds a user called <literal>tux</literal> to the
      subgroup <literal>nuremberg</literal>:
     </para>
<screen>&prompt.mgmt;sacctmgr add user tux Account=nuremberg</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-slurm-admin-commands">
  <title>&slurm; administration commands</title>
<!-- https://slurm.schedmd.com/man_index.html -->
  <para>
   This section lists some useful options for common &slurm; commands. For more
   information and a full list of options, see the <command>man</command> page
   for each command. For more &slurm; commands, see
   <link xlink:href="https://slurm.schedmd.com/man_index.html"/>.
  </para>
  <sect2 xml:id="sec-slurm-sconfigure">
   <title>scontrol</title>
   <para>
    The command <command>scontrol</command> is used to show and update the
    entities of &slurm;, such as the state of the compute nodes or compute jobs.
    It can also be used to reboot or to propagate configuration changes to the
    compute nodes.
   </para>
   <para>
    Useful options for this command are <option>--details</option>, which
    prints more verbose output, and <option>--oneliner</option>, which forces
    the output onto a single line, which is more useful in shell scripts.
   </para>
   <para>
    For more information, see <command>man scontrol</command>.
   </para>
   <variablelist>
    <varlistentry>
     <term><command>scontrol show <replaceable>ENTITY</replaceable></command></term>
     <listitem>
      <para>
       Displays the state of the specified <replaceable>ENTITY</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>scontrol update <replaceable>SPECIFICATION</replaceable></command></term>
     <listitem>
      <para>
       Updates the <replaceable>SPECIFICATION</replaceable> like the compute node
       or compute node state. Useful <replaceable>SPECIFICATION</replaceable>
       states that can be set for compute nodes include:
      </para>
      <variablelist>
       <varlistentry>
        <term><command>nodename=<replaceable>NODE</replaceable> state=down reason=<replaceable>REASON</replaceable></command></term>
        <listitem>
         <para>
          Removes all jobs from the compute node, and aborts any jobs already
          running on the node.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><command>nodename=<replaceable>NODE</replaceable> state=drain reason=<replaceable>REASON</replaceable></command></term>
        <listitem>
         <para>
          Drains the compute node so that no <emphasis>new</emphasis> jobs
          can be scheduled on it, but does not end compute jobs already
          running on the compute node. <replaceable>REASON</replaceable> can
          be any string. The compute node stays in the <literal>drained</literal>
          state and must be returned to the <literal>idle</literal> state manually.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><command>nodename=<replaceable>NODE</replaceable> state=resume</command></term>
        <listitem>
         <para>
          Marks the compute node as ready to return to the <literal>idle</literal>
          state.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><command>jobid=<replaceable>JOBID</replaceable>
         <replaceable>REQUIREMENT</replaceable>=<replaceable>VALUE</replaceable></command></term>
        <listitem>
         <para>
          Updates the given requirement, such as <literal>NumNodes</literal>,
          with a new value. This command can also be run as a non-privileged user.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>scontrol reconfigure</command></term>
     <listitem>
      <para>
       Triggers a reload of the configuration file
       <filename>slurm.conf</filename> on all compute nodes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>scontrol reboot <replaceable>NODELIST</replaceable></command></term>
     <listitem>
      <para>
       Reboots a compute node, or group of compute nodes, when the jobs on
       it finish. To use this command, the option
       <literal>RebootProgram="/sbin/reboot"</literal> must be set in
       <filename>slurm.conf</filename>. When the reboot of a compute node takes
       more than 60 seconds, you can set a higher value in
       <filename>slurm.conf</filename>, such as <literal>ResumeTimeout=300</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  <sect2 xml:id="sec-slurm-sinfo">
   <title>sinfo</title>
   <para>
    The command <command>sinfo</command> retrieves information about the state
    of the compute nodes, and can be used for a fast overview of the cluster
    health. For more information, see <command>man sinfo</command>.
   </para>
   <variablelist>
    <varlistentry>
     <term><command>--dead</command></term>
     <listitem>
      <para>
       Displays information about unresponsive nodes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--long</command></term>
     <listitem>
      <para>
       Shows more detailed information.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--reservation</command></term>
     <listitem>
      <para>
       Prints information about advanced reservations.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>-R</command></term>
     <listitem>
      <para>
       Displays the reason a node is in the <literal>down</literal>,
       <literal>drained</literal>, or <literal>failing</literal> state.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--state=<replaceable>STATE</replaceable></command></term>
     <listitem>
      <para>
       Limits the output only to nodes with the specified
       <replaceable>STATE</replaceable>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  <sect2 xml:id="sec-slurm-sacctmgr-sacct">
   <title>sacctmgr and sacct</title>
   <para>
    These commands are used for managing accounting. For more information, see
    <command>man sacctmgr</command> and <command>man sacct</command>.
   </para>
   <variablelist>
    <varlistentry>
     <term><command>sacctmgr</command></term>
     <listitem>
      <para>
       Used for job accounting in &slurm;. To use this command, the service
       <literal>slurmdbd</literal> must be set up. See
       <xref linkend="sec-slurm-slurmdbd"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>sacct</command></term>
     <listitem>
      <para>
       Displays the accounting data if accounting is enabled.
      </para>
      <variablelist>
       <varlistentry>
        <term><command>--allusers</command></term>
        <listitem>
         <para>
          Shows accounting data for all users.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><command>--accounts</command>=<replaceable>NAME</replaceable></term>
        <listitem>
         <para>
          Shows only the specified user(s).
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><command>--starttime</command>=<replaceable>MM/DD[/YY]-HH:MM[:SS]</replaceable></term>
        <listitem>
         <para>
          Shows only jobs after the specified start time. You can use just
          <replaceable>MM/DD</replaceable> or <replaceable>HH:MM</replaceable>.
          If no time is given, the command defaults to <literal>00:00</literal>,
          which means that only jobs from today are shown.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><command>--endtime</command>=<replaceable>MM/DD[/YY]-HH:MM[:SS]</replaceable></term>
        <listitem>
         <para>
          Accepts the same options as <command>--starttime</command>. If no time
          is given, the time when the command was issued is used.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><command>--name</command>=<replaceable>NAME</replaceable></term>
        <listitem>
         <para>
          Limits output to jobs with the given <replaceable>NAME</replaceable>.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><command>--partition</command>=<replaceable>PARTITION</replaceable></term>
        <listitem>
         <para>
          Shows only jobs that run in the specified <replaceable>PARTITION</replaceable>.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  <sect2 xml:id="sec-slurm-sbatch-salloc-srun">
   <title>sbatch, salloc, and srun</title>
   <para>
    These commands are used to schedule <emphasis>compute jobs</emphasis>,
    which means batch scripts for the <command>sbatch</command> command,
    interactive sessions for the <command>salloc</command> command, or
    binaries for the <command>srun</command> command. If the job cannot be
    scheduled immediately, only <command>sbatch</command> places it into the queue.
   </para>
   <para>
    For more information, see <command>man sbatch</command>,
    <command>man salloc</command>, and <command>man srun</command>.
   </para>
   <variablelist>
    <varlistentry>
     <term><command>-n <replaceable>COUNT_THREADS</replaceable></command></term>
     <listitem>
      <para>
       Specifies the number of threads needed by the job. The threads can be
       allocated on different nodes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>-N <replaceable>MINCOUNT_NODES[-MAXCOUNT_NODES]</replaceable></command></term>
     <listitem>
      <para>
       Sets the number of compute nodes required for a job. The
       <replaceable>MAXCOUNT_NODES</replaceable> number can be omitted.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--time <replaceable>TIME</replaceable></command></term>
     <listitem>
      <para>
       Specifies the maximum clock time (runtime) after which a job is
       terminated. The format of <replaceable>TIME</replaceable> is either
       seconds or <replaceable>[HH:]MM:SS</replaceable>. Not to be confused
       with <command>walltime</command>, which is <literal>clocktime &times;
       threads</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--signal <replaceable>[B:]NUMBER[@TIME]</replaceable></command></term>
     <listitem>
      <para>
       Sends the signal specified by <replaceable>NUMBER</replaceable>
       60 seconds before the end of the job, unless
       <replaceable>TIME</replaceable> is specified. The signal is
       sent to every process on every node. If a signal should only be sent
       to the controlling batch job, you must specify the
       <command>B:</command> flag.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--job-name <replaceable>NAME</replaceable></command></term>
     <listitem>
      <para>
       Sets the name of the job to <replaceable>NAME</replaceable> in the
       queue.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--array=<replaceable>RANGEINDEX</replaceable></command></term>
     <listitem>
      <para>
       Executes the given script via <command>sbatch</command> for indexes
       given by <replaceable>RANGEINDEX</replaceable> with the same
       parameters.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--dependency=<replaceable>STATE:JOBID</replaceable></command></term>
     <listitem>
      <para>
       Defers the job until the specified <replaceable>STATE</replaceable> of
       the job <replaceable>JOBID</replaceable> is reached.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--gres=<replaceable>GRES</replaceable></command></term>
     <listitem>
      <para>
       Runs a job only on nodes with the specified <emphasis>generic
       resource</emphasis> (GRes), for example a GPU, specified by the value
       of <replaceable>GRES</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--licenses=<replaceable>NAME[:COUNT]</replaceable></command></term>
     <listitem>
      <para>
       The job must have the specified number (<replaceable>COUNT</replaceable>)
       of licenses with the name <replaceable>NAME</replaceable>.
       A license is the opposite of a generic resource: it is not tied to a
       computer, but is a cluster-wide variable.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--mem=<replaceable>MEMORY</replaceable></command></term>
     <listitem>
      <para>
       Sets the real <replaceable>MEMORY</replaceable> required by a
       job per node. To use this option, memory control must be enabled. The
       default unit for the <replaceable>MEMORY</replaceable> value is
       megabytes, but you can also use <literal>K</literal> for kilobyte,
       <literal>M</literal> for megabyte, <literal>G</literal> for gigabyte,
       or <literal>T</literal> for terabyte.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>--mem-per-cpu=<replaceable>MEMORY</replaceable></command></term>
     <listitem>
      <para>
       This option takes the same values as <command>--mem</command>, but defines
       memory on a per-CPU basis rather than a per-node basis.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <!--sect1 xml:id="sec-pam-slurm-adopt">
  <title>Enabling the <literal>pam_slurm_adopt</literal> module</title>
  <para>
   The <literal>pam_slurm_adopt</literal> module allows restricting access to
   compute nodes to those users that have jobs running on them. It can also
   take care of <emphasis>run-away processes</emphasis> from users' jobs and
   end these processes when the jobs have finished.
  </para>
  <para>
   <literal>pam_slurm_adopt</literal> works by binding the login process
   of a user and all its child processes to the <literal>cgroup</literal>
   of a running job.
  </para>
  <para>
   It can be enabled with following steps:
  </para>
  <procedure>
   <step>
    <para>
     In the configuration file <filename>slurm.conf</filename>, set the option
     <literal>PrologFlags=contain</literal>.
    </para>
    <para>
     Make sure the option <literal>ProctrackType=proctrack/cgroup</literal>
     is also set.
    </para>
   </step>
   <step>
    <para>
     Restart the services
     <systemitem class="daemon">slurmctld</systemitem> and
     <systemitem class="daemon">slurmd</systemitem>.
    </para>
    <para>
     For this change to take effect, it is not sufficient to issue the command
     <command>scontrol reconfigure</command>.
    </para>
   </step>
   <step>
    <!- FIXME: Does limiting apply to CPU use only? ->
    <para>
     Decide whether to limit resources:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If resources are not limited, user processes can continue running on
       a node even after the job to which they were bound has finished.
      </para>
     </listitem>
     <listitem>
      <para>
       If resources are limited using a <literal>cgroup</literal>, user
       processes will be killed when the job finishes, and the controlling
       <literal>cgroup</literal> is deactivated.
      </para>
      <para>
       To activate resource limits via a <literal>cgroup</literal>, in the
       file <filename>/etc/slurm/cgroup.conf</filename>, set the option
       <literal>ConstrainCores=yes</literal>.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Due to the complexity of accurately determining RAM requirements of jobs,
     limiting the RAM space is not recommended.
    </para>
   </step>
   <step>
    <para>
     Install the package <package>slurm-pam_slurm</package>:
    </para>
    <screen>zypper install slurm-pam_slurm</screen>
   </step>
   <step performance="optional">
    <para>
     You can disallow logins by users who have no running jobs on the machine:
    </para>
    <itemizedlist>
     <listitem>
      <formalpara>
       <title>Disabling SSH logins only:</title>
       <para>
        In the file <literal>/etc/pam.d/ssh</literal>, add the option:
       </para>
      </formalpara>
      <screen>account     required pam_slurm_adopt.so</screen>
     </listitem>
     <listitem>
      <formalpara>
       <title>Disabling all types of logins:</title>
       <para>
        In the file <filename>/etc/pam.d/common-account</filename>, add the
        option:
       </para>
      </formalpara>
      <screen>account    required pam_slurm_adopt.so</screen>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1-->
 <sect1 xml:id="sec-slurm-upgrade">
  <title>Upgrading &slurm;</title>
   <para>
    For existing products under general support, version upgrades of &slurm; are
    provided regularly. Unlike maintenance updates, these upgrades are not
    installed automatically using <literal>zypper patch</literal> but require
    you to request their installation explicitly. This ensures that these
    upgrades are not installed unintentionally and gives you the opportunity
    to plan version upgrades beforehand.
   </para>
   <important>
     <title><command>zypper up</command> is not recommended</title>
     <para>
       On systems running &slurm;, updating packages with <command>zypper up</command>
       is not recommended. <command>zypper up</command> attempts to update all installed
       packages to the latest version, so might install new versions of &slurm; outside of
       planned &slurm; upgrades.
     </para>
     <para>
       Use <command>zypper patch</command> instead, which only updates packages to the
       latest bug fix version.
     </para>
   </important>
  <sect2 xml:id="sec-slurm-upgrade-workflow">
   <title>&slurm; upgrade workflow</title>
   <para>
    Interoperability is guaranteed between three consecutive versions of &slurm;,
    with the following restrictions:
   </para>
   <orderedlist>
    <listitem>
     <para>
      The version of <literal>slurmdbd</literal> must be identical to or higher
      than the version of <literal>slurmctld</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      The version of <literal>slurmctld</literal> must the identical to or
      higher than the version of <literal>slurmd</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      The version of <literal>slurmd</literal> must be identical to or higher
      than the version of the <literal>slurm</literal> user applications.
     </para>
    </listitem>
   </orderedlist>
   <para>
    Or in short:
    version(<literal>slurmdbd</literal>) &gt;=
    version(<literal>slurmctld</literal>) &gt;=
    version(<literal>slurmd</literal>) &gt;= version (&slurm; user CLIs).
   </para>
   <para>
    &slurm; uses a segmented version number: the first two segments denote the
    major version, and the final segment denotes the patch level.
    Upgrade packages (that is, packages that were not initially supplied with
    the module or service pack) have their major version encoded in the package
    name (with periods <literal>.</literal> replaced by underscores
    <literal>_</literal>). For example, for version 18.08, this would be
    <literal>slurm_18_08-*.rpm</literal>.
   </para>
   <para>
    With each version, configuration options for
    <literal>slurmctld</literal>, <literal>slurmd</literal>, or
    <literal>slurmdbd</literal> might be deprecated. While deprecated, they
    remain valid for this version and the two consecutive versions, but they might
    be removed later. Therefore, it is advisable to update the configuration files
    after the upgrade and replace deprecated configuration options before the
    final restart of a service.
   </para>
   <para>
    A new major version of &slurm; introduces a new version of
    <literal>libslurm</literal>. Older versions of this library might not work
    with an upgraded &slurm;. An upgrade is provided for all &sle; software that
    depends on <literal>libslurm </literal>. It is strongly recommended to rebuild
    local applications using <literal>libslurm</literal>, such as MPI libraries
    with &slurm; support, as early as possible. This might require updating the
    user applications, as new arguments might be introduced to existing functions.
   </para>
   <warning>
    <title>Upgrade <literal>slurmdbd</literal> databases before other &slurm; components</title>
    <para>
     If <literal>slurmdbd</literal> is used, always upgrade the
     <literal>slurmdbd</literal> database <emphasis>before</emphasis> starting
     the upgrade of any other &slurm; component. The same database can be connected
     to multiple clusters and must be upgraded before all of them.
    </para>
    <para>
     Upgrading other &slurm; components before the database can lead to data loss.
    </para>
   </warning>
  </sect2>
  <sect2 xml:id="sec-slurm-upgrade-database">
   <title>Upgrading the <literal>slurmdbd</literal> database daemon</title>
   <para>
    When upgrading <literal>slurmdbd</literal>,
    the database is converted when the new version of
    <literal>slurmdbd</literal> starts for the first time. If the
    database is big, the conversion could take several tens of minutes. During
    this time, the database is inaccessible.
   </para>
   <para>
    It is highly recommended to create a backup of the database in case an
    error occurs during or after the upgrade process. Without a backup,
    all accounting data collected in the database might be lost if an error
    occurs or the upgrade is rolled back. A database
    converted to a newer version cannot be converted back to an older version,
    and older versions of <literal>slurmdbd</literal> do not recognize the
    newer formats.
   </para>
   <note>
    <title>Convert primary <systemitem class="daemon">slurmdbd</systemitem> first</title>
    <para>
     If you are using a backup <literal>slurmdbd</literal>, the conversion must
     be performed on the primary <literal>slurmdbd</literal> first. The backup
     <literal>slurmdbd</literal> only starts after the conversion is complete.
    </para>
   </note>
   <procedure xml:id="pro-slurm-upgrade-database">
    <title>Upgrading the <literal>slurmdbd</literal> database daemon</title>
    <step>
     <para>
      Stop the <literal>slurmdbd</literal> service:
     </para>
<screen>&prompt.DBnode;rcslurmdbd stop</screen>
     <para>
      Ensure that <literal>slurmdbd</literal> is not running anymore:
     </para>
<screen>&prompt.DBnode;rcslurmdbd status</screen>
     <para>
      <literal>slurmctld</literal> might remain
      running while the database daemon is down. During this time, requests
      intended for <literal>slurmdbd</literal> are queued internally. The DBD
      Agent Queue size is limited, however, and should therefore be monitored
      with <literal>sdiag</literal>.
     </para>
    </step>
    <step>
     <para>
      Create a backup of the <literal>slurm_acct_db</literal> database:
     </para>
<screen>&prompt.DBnode;mysqldump -p slurm_acct_db &gt; slurm_acct_db.sql</screen>
     <para>
      If needed, this can be restored with the following command:
     </para>
<screen>&prompt.DBnode;mysql -p slurm_acct_db &lt; slurm_acct_db.sql</screen>
    </step>
    <step>
     <para>
      During the database conversion, the variable <literal>innodb_buffer_pool_size</literal>
      must be set to a value of 128&nbsp;MB or more. Check the current size:
     </para>
<screen>&prompt.DBnode;echo  'SELECT @@innodb_buffer_pool_size/1024/1024;' | \
mysql --password --batch</screen>
    </step>
    <step>
     <para>
      If the value of <literal>innodb_buffer_pool_size</literal> is less than
      128&nbsp;MB, you can change it for the duration of the current session
      (on <literal>mariadb</literal>):
     </para>
<screen>&prompt.DBnode;echo 'set GLOBAL innodb_buffer_pool_size = 134217728;' | \
mysql --password --batch</screen>
     <para>
      Alternatively, to permanently change the size, edit the <filename>/etc/my.cnf</filename>
      file, set <literal>innodb_buffer_pool_size</literal> to 128&nbsp;MB,
      then restart the database:
     </para>
<screen>&prompt.DBnode;rcmysql restart</screen>
    </step>
    <step>
     <para>
      If you need to update &mariadb;, run the following command:
     </para>
<screen>&prompt.DBnode;zypper update mariadb</screen>
     <para>
      Convert the database tables to the new version of &mariadb;:
     </para>
<screen>&prompt.DBnode;mysql_upgrade --user=root --password=<replaceable>ROOT_DB_PASSWORD</replaceable>;</screen>
    </step>
    <step>
     <para>
      Install the new version of <literal>slurmdbd</literal>:
     </para>
<screen>&prompt.DBnode;zypper install --force-resolution slurm_<replaceable>VERSION</replaceable>-slurmdbd</screen>
    </step>
    <step>
     <para>
      Rebuild the database. If you are using a backup <literal>slurmdbd</literal>,
      perform this step on the primary <literal>slurmdbd</literal> first.
     </para>
     <para>
      Because a conversion might take a considerable amount of time, the
      <literal>systemd</literal> service might time out during the conversion.
      Therefore, we recommend performing the migration manually by running
      <literal>slurmdbd</literal> from the command line in the foreground:
     </para>
<screen>&prompt.DBnode;/usr/sbin/slurmdbd -D -v</screen>
     <para>
      When you see the following message, you can shut down
      <literal>slurmdbd</literal> by pressing
      <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>:
     </para>
<screen>Conversion done:
success!</screen>
    </step>
    <step>
     <para>
      Before restarting the service, remove or replace any deprecated
      configuration options. Check the deprecated options in the
      <link xlink:href="&rn-url;"><citetitle>Release Notes</citetitle></link>.
     </para>
    </step>
    <step>
     <para>
      Restart <literal>slurmdbd</literal>:
     </para>
<screen>&prompt.DBnode;systemctl start slurmdbd</screen>
     <note>
      <title>No daemonization during rebuild</title>
      <para>
       During the rebuild of the &slurm; database, the database daemon does not
       daemonize.
      </para>
     </note>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec-slurm-upgrade-controller">
   <title>Upgrading <literal>slurmctld</literal> and <literal>slurmd</literal></title>
   <para>
    After the &slurm; database is upgraded, the <literal>slurmctld</literal> and
    <literal>slurmd</literal> instances can be upgraded. It is recommended to
    update the management servers and compute nodes all at once.
    If this is not feasible, the compute nodes (<literal>slurmd</literal>) can
    be updated on a node-by-node basis. However, the management servers
    (<literal>slurmctld</literal>) must be updated first.
   </para>
   <itemizedlist>
    <title>Prerequisites</title>
    <listitem>
     <para>
      <xref linkend="sec-slurm-upgrade-database"/>. Upgrading other &slurm;
      components before the database can lead to data loss.
     </para>
    </listitem>
    <listitem>
     <para>
      This procedure assumes that &munge; authentication is used and that
      <literal>pdsh</literal>, the <literal>pdsh</literal> &slurm; plugin, and
      <literal>&mrsh;</literal> can access all of the machines in the cluster.
      If this is not the case, install <literal>pdsh</literal> by running
      <command>zypper in pdsh-slurm</command>.
     </para>
     <para>
      If <literal>&mrsh;</literal> is not used in the cluster, the
      <literal>ssh</literal> back-end for <literal>pdsh</literal> can be used
      instead. Replace the option <literal>-R mrsh</literal> with
      <literal>-R ssh</literal> in the <literal>pdsh</literal>commands below. This
      is less scalable and you might run out of usable ports.
     </para>
    </listitem>
   </itemizedlist>
   <procedure xml:id="pro-slurm-upgrade-controller">
    <title>Upgrading <literal>slurmctld</literal> and <literal>slurmd</literal></title>
    <step>
     <para>
      Back up the configuration file<filename>/etc/slurm/slurm.conf</filename>.
      Because this file should be identical across the entire cluster, it is
      sufficient to do so only on the main management server.
     </para>
    </step>
    <step>
     <para>
      On the main management server, edit <filename>/etc/slurm/slurm.conf</filename>
      and set <literal>SlurmdTimeout</literal> and <literal>SlurmctldTimeout</literal>
      to sufficiently high values to avoid timeouts while <literal>slurmctld</literal>
      and <literal>slurmd</literal> are down:
     </para>
<screen>SlurmctldTimeout=3600
SlurmdTimeout=3600</screen>
     <para>
      We recommend at least 60 minutes (<literal>3600</literal>), and more for
      larger clusters.
     </para>
    </step>
    <step xml:id="st-copy-file-to-nodes">
     <para>
      Copy the updated <filename>/etc/slurm/slurm.conf</filename> from the
      management server to all nodes:
     </para>
     <substeps>
      <step>
       <para>
        Obtain the list of partitions in
        <filename>/etc/slurm/slurm.conf</filename>.
       </para>
      </step>
      <step>
       <para>
        Copy the updated configuration to the compute nodes:
       </para>
<screen>&prompt.mgmt;cp /etc/slurm/slurm.conf /etc/slurm/slurm.conf.update
&prompt.mgmt;sudo -u slurm /bin/bash -c 'cat /etc/slurm/slurm.conf.update | \
pdsh -R mrsh -P <replaceable>PARTITIONS</replaceable> "cat > /etc/slurm/slurm.conf"'
&prompt.mgmt;rm /etc/slurm/slurm.conf.update
</screen>
      </step>
      <step>
       <para>
        Reload the configuration file on all compute nodes:
       </para>
<screen>&prompt.mgmt;scontrol reconfigure</screen>
      </step>
      <step>
       <para>
        Verify that the reconfiguration took effect:
       </para>
<screen>&prompt.mgmt;scontrol show config | grep Timeout</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Shut down all running <literal>slurmctld</literal> instances, first on
      any backup management servers, and then on the main management server:
     </para>
<screen>&prompt.mgmt;systemctl stop slurmctld</screen>
    </step>
    <step>
     <para>
      Back up the <literal>slurmctld</literal> state files.
      <literal>slurmctld</literal> maintains persistent state information.
      Almost every major version involves changes to the <literal>slurmctld</literal>
      state files. This state information is upgraded if the upgrade remains
      within the supported version range and no data is lost.
     </para>
     <para>
      However, if a downgrade is necessary, state information from
      newer versions is not recognized by an older version of
      <literal>slurmctld</literal> and is discarded, resulting in a
      loss of all running and pending jobs. Therefore, back up the
      old state in case an update needs to be rolled back.
     </para>
     <substeps>
      <step>
       <para>
        Determine the <literal>StateSaveLocation</literal> directory:
       </para>
<screen>&prompt.mgmt;scontrol show config | grep StateSaveLocation</screen>
      </step>
      <step>
       <para>
        Create a backup of the content of this directory. If a downgrade is
        required, restore the content of
        the <literal>StateSaveLocation</literal> directory from this backup.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Shut down <literal>slurmd</literal> on the compute nodes:
     </para>
<screen>&prompt.mgmt;pdsh -R ssh -P <replaceable>PARTITIONS</replaceable> systemctl stop slurmd</screen>
    </step>
    <step>
     <para>
      Upgrade <literal>slurmctld</literal> on the main and backup management
      servers:
     </para>
<screen>&prompt.mgmt;zypper install --force-resolution slurm_<replaceable>VERSION</replaceable></screen>
     <important>
      <title>Upgrade all &slurm; packages at the same time</title>
      <para>
       If any additional &slurm; packages are installed, you must upgrade
       those as well. This includes:
      </para>
       <itemizedlist>
        <listitem>
         <para>
          slurm-pam_slurm
         </para>
        </listitem>
        <listitem>
         <para>
          slurm-sview
         </para>
        </listitem>
        <listitem>
         <para>
          perl-slurm
         </para>
        </listitem>
        <listitem>
         <para>
          slurm-lua
         </para>
        </listitem>
        <listitem>
         <para>
          slurm-torque
         </para>
        </listitem>
        <listitem>
         <para>
          slurm-config-man
         </para>
        </listitem>
        <listitem>
         <para>
          slurm-doc
         </para>
        </listitem>
        <listitem>
         <para>
          slurm-webdoc
         </para>
        </listitem>
        <listitem>
         <para>
          slurm-auth-none
         </para>
        </listitem>
        <listitem>
         <para>
          pdsh-slurm
         </para>
        </listitem>
       </itemizedlist>
       <para>
        All &slurm; packages must be upgraded at the same time to avoid conflicts
        between packages of different versions. This can be done by adding them to
        the <literal>zypper install</literal> command line described above.
      </para>
     </important>
    </step>
    <step>
     <para>
      Upgrade <literal>slurmd</literal> on the compute nodes:
     </para>
<screen>&prompt.mgmt;pdsh -R ssh -P <replaceable>PARTITIONS</replaceable> \
zypper install --force-resolution slurm_<replaceable>VERSION</replaceable>-node</screen>
     <note>
      <title>Memory size seen by <literal>slurmd</literal> might change on update</title>
      <para>
       Under certain circumstances, the amount of memory seen by
       <literal>slurmd</literal> might change after an update. If this happens,
       <literal>slurmctld</literal> puts the nodes in a
       <literal>drained</literal> state. To check whether the amount of
       memory seem by <literal>slurmd</literal> changed after the update,
       run the following command on a single compute node:
      </para>
<screen>&prompt.node1;slurmd -C</screen>
      <para>
       Compare the output with the settings in
       <filename>slurm.conf</filename>. If required, correct the setting.
      </para>
     </note>
    </step>
    <step>
     <para>
      Before restarting the service, remove or replace any deprecated
      configuration options. Check the deprecated options in the
      <link xlink:href="&rn-url;"><citetitle>Release Notes</citetitle></link>.
     </para>
     <para>
      If you replace deprecated options in the configuration files, these
      configuration files can be distributed to all management servers and
      compute nodes in the cluster by using the method described in
      <xref linkend="st-copy-file-to-nodes"/>.
     </para>
    </step>
    <step>
     <para>
      Restart <literal>slurmd</literal> on all compute nodes:
     </para>
<screen>&prompt.mgmt;pdsh -R ssh -P <replaceable>PARTITIONS</replaceable> systemctl start slurmd</screen>
    </step>
    <step>
     <para>
      Restart <literal>slurmctld</literal> on the main and backup management servers:
     </para>
<screen>&prompt.mgmt;systemctl start slurmctld</screen>
    </step>
    <step>
     <para>
      Check the status of the management servers. On the main and backup
      management servers, run the following command:
     </para>
<screen>&prompt.mgmt;systemctl status slurmctld</screen>
    </step>
    <step>
     <para>
      Verify that the services are running without errors. Run the
      following command to check whether there are any <literal>down</literal>,
      <literal>drained</literal>, <literal>failing</literal>, or
      <literal>failed</literal> nodes:
     </para>
<screen>&prompt.mgmt;sinfo -R</screen>
    </step>
    <step>
     <para>
      Restore the original values of <literal>SlurmdTimeout</literal> and
      <literal>SlurmctldTimeout</literal> in <literal>/etc/slurm/slurm.conf</literal>,
      then copy the restored configuration to all nodes by using the method
      described in <xref linkend="st-copy-file-to-nodes"/>.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-slurm-faq">
   <title>Frequently asked questions</title>
   <qandaset>
    <qandaentry>
     <question>
      <para>
       How do I change the state of a node from <literal>down</literal> to
       <literal>up</literal>?
      </para>
     </question>
     <answer>
      <para>
       When the <literal>slurmd</literal> daemon on a node does not reboot in
       the time specified in the <literal>ResumeTimeout</literal> parameter, or
       the <literal>ReturnToService</literal> was not changed in the
       configuration file <filename>slurm.conf</filename>, compute nodes stay
       in the <literal>down</literal> state and must be set back to the
       <literal>up</literal> state manually. This can be done for the
       <replaceable>NODE</replaceable> with the following command:
      </para>
<screen>&prompt.mgmt;scontrol update state=resume NodeName=<replaceable>NODE</replaceable></screen>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What is the difference between the states <literal>down</literal> and
       <literal>down*</literal>?
      </para>
     </question>
     <answer>
      <para>
       A <literal>*</literal> shown after a status code means that the node is
       not responding.
      </para>
      <para>
       When a node is marked as <literal>down*</literal>, it means that
       the node is not reachable because of network issues, or that
       <literal>slurmd</literal> is not running on that node.
      </para>
      <para>
       In the <literal>down</literal> state, the node is reachable, but either
       the node was rebooted unexpectedly, the hardware does not match the
       description in <filename>slurm.conf</filename>, or a health check was
       configured with the <literal>HealthCheckProgram</literal>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       How do I get the exact core count, socket number, and number of CPUs for
       a node?
      </para>
     </question>
     <answer>
      <para>
       To find the node values that go into the configuration file
       <filename>slurm.conf</filename>, run the following command:
      </para>
<screen>&prompt.node1;slurmd -C</screen>
     </answer>
    </qandaentry>
   </qandaset>
 </sect1>
</chapter>
