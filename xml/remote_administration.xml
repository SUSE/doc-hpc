<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-remote" xml:lang="en"
         xmlns="http://docbook.org/ns/docbook" version="5.1"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Remote administration</title>
 <info>
  <abstract>
   <para>
    &hpc; clusters usually consist of a small set of identical compute
    nodes. However, large clusters could consist of thousands of machines.
    This chapter describes tools to help manage the compute nodes in a cluster.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-remote-genders">
  <title>Genders &mdash; static cluster configuration database</title>
  <para>
   <emphasis>Genders</emphasis> is a static cluster configuration database used
   for configuration management. It allows grouping and addressing sets of
   nodes by attributes, and is used by a variety of tools. The Genders
   database is a text file that is usually replicated on each node in a
   cluster.
  </para>
  <para>
   Perl, Python, Lua, C, and C++ bindings are supplied with Genders. Each
   package provides <command>man</command> pages or other documentation which
   describes the APIs.
  </para>
  <sect2 xml:id="sec-genders-db">
    <title>Genders database format</title>
  <para>
   The Genders database in &slehpc; is a plain-text file called
   <filename>/etc/genders</filename>. It contains a list of node names with
   their attributes. Each line of the database can have one of the following
   formats.
  </para>
    <screen>nodename                attr[=value],attr[=value],...
nodename1,nodename2,... attr[=value],attr[=value],...
nodenames[A-B]          attr[=value],attr[=value],...</screen>
  <para>
   Node names are listed without their domain, and are followed by any number of
   spaces or tabs, then the comma-separated list of attributes. Every
   attribute can optionally have a value. The substitution string
   <literal>%n</literal> can be used in an attribute value to represent the node
   name. Node names can be listed on multiple lines, so a node's attributes can
   be specified on multiple lines. However, no single node can have duplicate
   attributes.
  </para>
  <para>
   The attribute list must not contain spaces, and there is no provision for
   continuation lines.  Commas and equals characters (<literal>=</literal>) are
   special, and cannot appear in attribute names or values. Comments are
   prefixed with the hash character (<literal>#</literal>) and can appear
   anywhere in the file.
  </para>
  <para>
   Ranges for node names can be specified in the form
   <literal>prefix[a-c,n-p,...]</literal> as an alternative to explicit lists
   of node names. For example, <literal>node[01-03,06]</literal> would specify
   <literal>node01</literal>, <literal>node02</literal>, <literal>node03</literal>,
   and <literal>node06</literal>.
  </para>
  </sect2>

  <sect2 xml:id="sec-nodeattr">
   <title>Nodeattr usage</title>
   <!-- REVISIT: This content is all available from the nodeattr man page-->
   <para>
    The command line utility <command>nodeattr</command> can be used to query data
    in the genders file. When the genders file is replicated on all nodes, a query
    can be done without network access. The genders file can be called as follows:
   </para>
<screen>&prompt;nodeattr [-q | -n | -s] [-r] attr[=val]</screen>
  <para>
   <literal>-q</literal> is the default option and prints a list of nodes
   with <literal>attr[=val]</literal>.
  </para>
  <para>
   The <literal>-c</literal> or <literal>-s</literal> options give a
   comma-separated or space-separated list of nodes with
   <literal>attr[=val]</literal>.
  </para>
  <para>
   If none of the formatting options are specified, <command>nodeattr</command>
   returns a zero value if the local node has the specified attribute, and
   non-zero otherwise. The <literal>-v</literal> option causes any value
   associated with the attribute to go to <literal>stdout</literal>. If a node
   name is specified before the attribute, the specified node is queried instead
   of the local node.
  </para>
  <para>
    To print all attributes for a particular node, run the following command:
  </para>
<screen>&prompt;nodeattr -l [node]</screen>
  <para>
   If no node parameter is given, all attributes of the local node are printed.
  </para>
  <para>
   To perform a syntax check of the genders database, run the following command:
  </para>
<screen>&prompt;nodeattr [-f genders] -k</screen>
  <para>
   To specify an alternative database location, use the option
   <literal>-f</literal>.
  </para>
 </sect2>

 </sect1>
 <sect1 xml:id="sec-remote-pdsh">
  <title>pdsh &mdash; parallel remote shell program</title>
  <para>
   <command>pdsh</command> is a parallel remote shell that can be used
   with multiple back-ends for remote connections. It can run a command on
   multiple machines in parallel.
  </para>
  <para>
   To install <package>pdsh</package>, run the command
   <command>zypper in pdsh</command>.
  </para>
  <para>
   In &slehpc;, the back-ends <literal>ssh</literal>,
   <literal>&mrsh;</literal>, and <literal>exec</literal> are supported. The
   <literal>ssh</literal> back-end is the default. Non-default login methods
   can be used by setting the <literal>PDSH_RCMD_TYPE</literal>
   environment variable, or by using the <literal>-R</literal> command
   argument.
  </para>
  <para>
   When using the <literal>ssh</literal> back-end, you must use a
   non-interactive (passwordless) login method.
  </para>
  <para>
   The <literal>&mrsh;</literal> back-end requires the
   <literal>mrshd</literal> daemon to be running on the client. The
   <literal>&mrsh;</literal> back-end does not require the use of reserved
   sockets, so it does not suffer from port exhaustion when running commands
   on many machines in parallel. For information about setting up the system to
   use this back-end, see <xref linkend="sec-remote-mrsh"/>.
  </para>
  <para>
   Remote machines can be specified on the command line, or
   <command>pdsh</command> can use a <filename>machines</filename> file
   (<filename>/etc/pdsh/machines</filename>), <command>dsh</command> (Dancer's
   shell)-style groups or netgroups. It can also target nodes based on the
   currently running &slurm; jobs.
  </para>
  <para>
   The different ways to select target hosts are realized by modules. Some
   of these modules provide identical options to <command>pdsh</command>.
   The module loaded first will win and handle the option. Therefore, we
   recommended using a single method and specifying this with
   the <literal>-M</literal> option.
  </para>
  <para>
   The <filename>machines</filename> file lists all target hosts, one per
   line. The appropriate netgroup can be selected with the
   <literal>-g</literal> command line option.
  </para>
  <para>
   The following host-list plugins for <command>pdsh</command> are supported:
   <literal>machines</literal>, <literal>slurm</literal>,
   <literal>netgroup</literal> and <literal>dshgroup</literal>.
   Each host-list plugin is provided in a separate package. This avoids
   conflicts between command line options for different plugins which
   happen to be identical, and helps to keep installations small and free
   of unneeded dependencies. Package dependencies have been set to prevent
   the installation of plugins with conflicting command options. To install one
   of the plugins, run:
  </para>
<screen>&prompt;zypper in pdsh-<replaceable>PLUGIN_NAME</replaceable></screen>
  <para>
   For more information, see the <command>man</command> page <command>pdsh</command>.
  </para>
 </sect1>
 <sect1 xml:id="sec-remote-powerman">
  <title>PowerMan &mdash; centralized power control for clusters</title>
  <para>
   PowerMan can control the following remote power control devices (RPC) from a
   central location:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     local devices connected to a serial port
    </para>
   </listitem>
   <listitem>
    <para>
     RPCs listening on a TCP socket
    </para>
   </listitem>
   <listitem>
    <para>
     RPCs that are accessed through an external program
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The communication to RPCs is controlled by <quote>expect</quote>-like
   scripts. For a
   list of currently supported devices, see the configuration file
   <filename>/etc/powerman/powerman.conf</filename>.
  </para>
  <para>
   To install PowerMan, run <command>zypper in powerman</command>.
  </para>
  <para>
   To configure PowerMan, include the appropriate device file for your RPC
   (<filename>/etc/powerman/*.dev</filename>) in
   <filename>/etc/powerman/powerman.conf</filename> and add devices and
   nodes. The device <quote>type</quote> needs to match the
   <quote>specification</quote> name in one
   of the included device files. The list of <quote>plugs</quote> used for
   nodes needs to
   match an entry in the <quote>plug name</quote> list.
  </para>
  <para>
   After configuring PowerMan, start its service:
  </para>
<screen>&prompt;systemctl start powerman.service</screen>
  <para>
   To start PowerMan automatically after every boot, run the following command:
  </para>
<screen>&prompt;systemctl enable powerman.service</screen>
  <para>
   Optionally, PowerMan can connect to a remote PowerMan instance. To
   enable this, add the option <literal>listen</literal> to
   <filename>/etc/powerman/powerman.conf</filename>.
  </para>
  <important>
   <title>Unencrypted transfer</title>
   <para>
    When connecting to a remote PowerMan instance, data is transferred
    unencrypted. Therefore, use this feature only if the network is
    appropriately secured.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="sec-remote-munge">
  <title>&munge; authentication</title>
  <para>
   &munge; allows for secure communications between different machines that
   share the same secret key. The most common use case is the &slurm; workload
   manager, which uses &munge; for the encryption of its messages. Another use
   case is authentication for the parallel shell &mrsh;.
  </para>
  <sect2 xml:id="sec-setting-up-munge">
   <title>Setting up &munge; authentication</title>
   <para>
    &munge; uses UID/GID values to uniquely identify and authenticate users,
    so you must ensure that users who will authenticate across a network
    have matching UIDs and GIDs across all nodes.
   </para>
   <para>
    &munge; credentials have a limited time-to-live, so you must ensure that the
    time is synchronized across the entire cluster.
   </para>
   <para>
    &munge; is installed with the command <command>zypper in munge</command>.
    This also installs further required packages. A separate
    <package>munge-devel</package> package is available to build applications
    that require &munge; authentication.
   </para>
   <para>
    When installing the <package>munge</package> package, a new key is generated
    on every system. However, the entire cluster needs to use the same &munge;
    key. Therefore, you must securely copy the &munge; key from one system to
    all the other nodes in the cluster. You can accomplish this by using
    <command>pdsh</command> with SSH. Ensure that the key is only readable
    by the <literal>munge</literal> user (permissions mask
    <literal>0400</literal>).
   </para>
   <procedure xml:id="pro-remote-munge">
    <title>Setting up &munge; authentication</title>
    <step>
     <para>
      On the server where &munge; is installed, check the permissions, owner,
      and file type of the key file <filename>/etc/munge/munge.key</filename>:
     </para>
<screen>&prompt;stat --format "%F %a %G %U %n" /etc/munge/munge.key</screen>
     <para>
      The settings should be as follows:
     </para>
<screen>400 regular file munge munge /etc/munge/munge.key</screen>
    </step>
    <step>
     <para>
      Calculate the MD5 sum of <filename>munge.key</filename>:
     </para>
<screen>&prompt;md5sum /etc/munge/munge.key</screen>
    </step>
    <step>
     <para>
      Copy the key to the listed nodes using <command>pdcp</command>:
     </para>
<screen>&prompt;pdcp -R ssh -w <replaceable>NODELIST</replaceable> /etc/munge/munge.key /etc/munge/munge.key</screen>
    </step>
    <step>
     <para>
      Check the key settings on the remote nodes:
     </para>
<screen>&prompt;pdsh -R ssh -w <replaceable>HOSTLIST</replaceable> stat --format \"%F %a %G %U %n\" /etc/munge/munge.key
&prompt;pdsh -R ssh -w <replaceable>HOSTLIST</replaceable> md5sum /etc/munge/munge.key</screen>
     <para>
      Ensure that they match the settings on the &munge; server.
     </para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec-enabling-and-starting-munge">
   <title>Enabling and starting &munge;</title>
   <para>
    <systemitem class="daemon">munged</systemitem> must be running on all nodes
    that use &munge; authentication. If &munge; is used for
    authentication across the network, it needs to run on each side of the
    communications link.
   </para>
   <para>
    To start the service and ensure it is started after every reboot, run
    the following command on each node:
   </para>
<screen>&prompt;systemctl enable --now munge.service</screen>
    <para>
     You can also use <command>pdsh</command> to run this command on multiple
     nodes at once.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-remote-mrsh">
  <title>&mrsh;/mrlogin &mdash; remote login using &munge; authentication</title>
  <para>
   <emphasis>&mrsh;</emphasis> is a set of remote shell programs using the
   <emphasis>&munge;</emphasis> authentication system instead of reserved ports
   for security.
  </para>
  <para>
   It can be used as a drop-in replacement for <literal>rsh</literal> and
   <literal>rlogin</literal>.
  </para>
  <para>
   To install &mrsh;, do the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     If only the &mrsh; client is required (without
     allowing remote login to this machine), use:
     <command>zypper in mrsh</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     To allow logging in to a machine, the server must be installed:
     <command>zypper in mrsh-server</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     To get a drop-in replacement for <command>rsh</command> and
     <command>rlogin</command>, run: <command>zypper in
     mrsh-rsh-server-compat</command> or <command>zypper in
     mrsh-rsh-compat</command>.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   To set up a cluster of machines allowing remote login from each other,
   first follow the instructions for setting up and starting &munge;
   authentication in <xref linkend="sec-remote-munge"/>. After the &munge; service
   successfully starts, enable and start <command>mrlogin</command>
   on each machine on which the user will log in:
  </para>
<screen>&prompt;systemctl enable mrlogind.socket mrshd.socket
&prompt;systemctl start mrlogind.socket mrshd.socket</screen>
  <para>
   To start &mrsh; support at boot, run the following command:
  </para>
<screen>&prompt;systemctl enable munge.service
&prompt;systemctl enable mrlogin.service</screen>
  <para>
   We do not recommend using &mrsh; when logged in as the
   user <systemitem class="username">root</systemitem>. This is disabled by
   default. To enable it anyway, run the following command:
  </para>
  <screen>&prompt;echo "mrsh" &gt;&gt; /etc/securetty
&prompt;echo "mrlogin" &gt;&gt; /etc/securetty</screen>
 </sect1>
</chapter>
