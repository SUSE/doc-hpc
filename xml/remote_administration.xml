<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-remote" xml:lang="en"
         xmlns="http://docbook.org/ns/docbook" version="5.1"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Remote administration</title>
 <info>
  <abstract>
   <para>
    HPC clusters usually consist of a small set of identical compute
    nodes. Each cluster may consist of a mere handful of machines, up to
    thousands. Managing the compute nodes of a cluster may be challenging. This
    chapter describes a number of tools which aid this task.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-remote-genders">
  <title>Genders &mdash; static cluster configuration database</title>
  <para>
   <emphasis>Genders</emphasis> is a static cluster configuration database used
   for configuration management. It allows grouping and addressing sets of
   hosts by attributes, and is used by a variety of tools. The Genders
   database is a text file which is usually replicated on each node in a
   cluster.
  </para>
  <para>
   Perl, Python, Lua, C, and C++ bindings are supplied with Genders. Each
   package provides <command>man</command> pages or other documentation which
   describes the APIs.
  </para>
  <sect2 xml:id="sec-genders-db">
    <title>Genders database format</title>
  <para>
   The Genders database in &shpca; is a plain-text file
   <filename>/etc/genders</filename>. It contains a list of node names with
   their attributes. Each line of the database may have one of the following
   formats.
  </para>
    <screen>
nodename                attr[=value],attr[=value],...
nodename1,nodename2,... attr[=value],attr[=value],...
nodenames[A-B]          attr[=value],attr[=value],...
    </screen>
  <para>
   Node names are listed without their domain, and are followed by any number of
   spaces or tabs, then the comma-separated list of attributes. Every
   attribute can optionally have a value. The substitution string
   <literal>%n</literal> can be used in an attribute value to represent the node
   name. Node names can be listed on multiple lines, so a node's attributes can
   be specified on multiple lines. However, no single node may have duplicate
   attributes.
  </para>
  <para>
   The attribute list must not contain spaces, and there is no provision for
   continuation lines.  Commas and equals characters (<literal>=</literal>) are
   special, and cannot appear in attribute names or values. Comments are
   prefixed with the hash character (<literal>#</literal>) and can appear
   anywhere in the file.
  </para>
  <para>
   Ranges for node names can be specified in the form:
   <literal>prefix[n-m,l-k,...]</literal>, where n &gt; m and l &gt; k, etc.,
   as an alternative to explicit lists of node names.
  </para>
  </sect2>
  
  <sect2 xml:id="sec-nodeattr">
   <title>Nodeattr usage</title>
   <para>
    The command line utility <command>nodeaddr</command> can be used to query data
    in the genders file. When the genders file is replicated on all nodes, a qery
    can be done wihtout network access. It can be called like followed
   </para>
<screen>nodeattr [-q | -n | -s] [-r] attr[=val]</screen>
  <para>
   where <literal>-q</literal> is the default option and prints a list nodes
   with <literal>attr[=val]</literal> as the host range.
  </para>
  <para>
   The <literal>-c</literal> options will give a comma-separated list, and the
   <literal>-s</literal> option a space-separated list, of nodes with 
   <literal>attr[=val]</literal>.
  </para>
  <para>
   If none of the formatting options are specified, <command>nodeattr</command>
   returns a zero value if the local node has the specified attribute, and
   non-zero otherwise. The <literal>-v</literal> option causes any value
   associated with the attribute to go to <literal>stdout</literal>. If a node
   name is specified before the attribute, it is queried instead of the
   local node.
  </para>
  <para>
    When <command>nodeattr</command> is called with the <literal>-l</literal>
    parameter, as follows:
  </para>
<screen>nodeattr -l [node]</screen>
  <para>
   all attributes for a particular node are printed out. If no node
   paramteter is is given, all attributes of the local node are printed out.
  </para>
  <para>
   With the command
  </para>
<screen>nodeattr [-f genders] -k</screen>
  <para>
   a syntax check of the genders database is performed, where with the switch
   <literal>-f</literal> an alternative datanbase location can be specified.
  </para>
 </sect2>
  
 </sect1>
 <sect1 xml:id="sec-remote-pdsh">
  <title>pdsh &mdash; parallel remote shell program</title>
  <para>
   <command>pdsh</command> is a parallel remote shell which can be used
   with multiple back-ends for remote connections. It can run a command on
   multiple machines in parallel.
  </para>
  <para>
   To install <package>pdsh</package>, run the command:
   <command>zypper in pdsh</command>.
  </para>
  <para>
   On &shpca; the back-ends <literal>ssh</literal>,
   <literal>mrsh</literal>, and <literal>exec</literal> are supported. The
   <literal>ssh</literal> back-end is the default. Non-default login methods
   can be used by either setting the <literal>PDSH_RCMD_TYPE</literal>
   environment variable or by using the <literal>-R</literal> command
   argument.
  </para>
  <para>
   When using the <literal>ssh</literal> back-end, it is important that a
   non-interactive (that is, passwordless) login method is used.
  </para>
  <para>
   The <literal>mrsh</literal> back-end requires the
   <literal>mrshd</literal> to be running on the client. The
   <literal>mrsh</literal> back-end does not require the use of reserved
   sockets. Therefore, it does not suffer from port exhaustion when
   executing commands on many machines in parallel. For information about
   setting up the system to use this back-end, see
   <xref linkend="sec-remote-mrsh"/>
  </para>
  <para>
   Remote machines can be specified on the command line, or
   <command>pdsh</command> can use a <filename>machines</filename> file
   (<filename>/etc/pdsh/machines</filename>), <command>dsh</command> (Dancer's
   shell)-style groups or netgroups. Also, it can target nodes based on the
   currently running Slurm jobs.
  </para>
  <para>
   The different ways to select target hosts are realized by modules. Some
   of these modules provide identical options to <command>pdsh</command>.
   The module loaded first will win and handle the option. Therefore, we
   recommended limiting yourself to a single method and specifying this with
   the <literal>-M</literal> option.
  </para>
  <para>
   The <filename>machines</filename> file lists all target hosts one per
   line. The appropriate netgroup can be selected with the
   <literal>-g</literal> command line option.
  </para>
  <para>
   The following host-list plugins for <command>pdsh</command> are supported:
   <literal>machines</literal>, <literal>slurm</literal>,
   <literal>netgroup</literal> and <literal>dshgroup</literal>.
   Each host-list plugin is provided in a separate package. This avoids
   conflicts between command-line options for different plugins which
   happen to be identical, and helps to keep installations small and free
   of unneeded dependencies. Package dependencies have been set to prevent
   the installation of plugins with conflicting command options. To install one
   of the plugins, run:
  </para>
<screen>zypper in pdsh-<replaceable>PLUGIN_NAME</replaceable></screen>
  <para>
   For more information, see the <command>man</command> page <command>pdsh</command>.
  </para>
 </sect1>
 <sect1 xml:id="sec-remote-powerman">
  <title>PowerMan &mdash; centralized power control for clusters</title>
  <para>
   PowerMan allows manipulating remote power control devices (RPC) from a
   central location. It can control:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     local devices connected to a serial port
    </para>
   </listitem>
   <listitem>
    <para>
     RPCs listening on a TCP socket
    </para>
   </listitem>
   <listitem>
    <para>
     RPCs which are accessed through an external program
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The communication to RPCs is controlled by <quote>expect</quote>-like
   scripts. For a
   list of currently supported devices, see the configuration file
   <filename>/etc/powerman/powerman.conf</filename>.
  </para>
  <para>
   To install PowerMan, run <command>zypper in powerman</command>.
  </para>
  <para>
   To configure it, include the appropriate device file for your RPC
   (<filename>/etc/powerman/*.dev</filename>) in
   <filename>/etc/powerman/powerman.conf</filename> and add devices and
   nodes. The device <quote>type</quote> needs to match the
   <quote>specification</quote> name in one
   of the included device files. The list of <quote>plugs</quote> used for
   nodes need to
   match an entry in the <quote>plug name</quote> list.
  </para>
  <para>
   After configuring PowerMan, start its service by:
  </para>
  <screen>systemctl start powerman.service</screen>
  <para>
   To start PowerMan automatically after every boot, run:
  </para>
  <screen>systemctl enable powerman.service</screen>
  <para>
   Optionally, PowerMan can connect to a remote PowerMan instance. To
   enable this, add the option <literal>listen</literal> to
   <filename>/etc/powerman/powerman.conf</filename>.
  </para>
  <important>
   <title>Unencrypted transfer</title>
   <para>
    When connecting to a remote PowerMan instance, data is transferred
    unencrypted. Therefore, use this feature only if the network is
    appropriately secured.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="sec-remote-munge">
  <title>MUNGE authentication</title>
  <para>
   <emphasis>MUNGE</emphasis> allows for secure communications between
   different machines which share the same secret key. The most common
   usecase is the <emphasis>Slurm</emphasis> workload manager, which
   uses MUNGE for the encryption of its messages. Another use case is
   authentication for the parallel shell
   <emphasis>mrsh</emphasis>.
  </para>
  <sect2>
   <title>Setting up MUNGE authentication</title>
   <para>
    MUNGE uses UID/GID values to uniquely identify and authenticate users. Thus,
    you must take care that users who will authenicate across a network have
    unified UIDs and GIDs.
   </para>
   <para>
    MUNGE credentials have a limited time-to-live, so you must ensure that the
    time is synchronized across the entire cluster.
   </para>
   <para>
    MUNGE is installed with the command <command>zypper in munge</command>.
    This will install all packages required at runtime. A
    <package>munge-devel</package> package is available to build applications
    that require <emphasis>munge</emphasis> authentication.
   </para>
   <para>
    When installing the <package>munge</package> package, a new key is generated
    on every system. However, this MUNGE key must be uniform across the entire
    cluster. Therefore, the key from one system needs to be copied in a secure
    way to all other nodes in the cluster. Care must be taken that the key is
    only readable by the <literal>munge</literal> user (permissions mask
    <literal>0400</literal>).
   </para>
   <para>
    <emphasis>pdsh</emphasis> (with ssh) may be used to do this:
   </para>
   <para>
    Check permissions, owner and file type of the key file located under
    <filename>/etc/munge/munge.key</filename> on the local system:
   </para>
<screen># stat --format "%F %a %G %U %n" /etc/munge/munge.key</screen>
   <para>The settings should be:</para>
<screen>400 regular file munge munge /etc/munge/munge.key</screen>
   <para>Get md5sum of munge.key:</para>
<screen># md5sum /etc/munge/munge.key</screen>
   <para>Copy key to list of nodes:</para>
<screen># pdcp -R ssh -w &lt;hostlist&gt; /etc/munge/munge.key
   /etc/munge/munge.key </screen>
   <para>Check key settings in remote host:</para>
<screen>pdsh -R ssh -w &lt;hostlist&gt; stat --format \"%F %a %G %U %n\"
/etc/munge/munge.key
pdsh -R ssh -w &lt;hostlist&gt; md5sum /etc/munge/munge.key</screen>
   <para>Make sure that they match.</para>
  </sect2>
  <sect2>
   <title>Enabling and starting MUNGE</title>
   <para>
    <systemitem class="daemon">munged</systemitem> needs to be run on all nodes
    where MUNGE authentication will take place. If MUNGE is used for
    authentication across the network, it needs to run on each side of the
    communications link.
   </para>
   <para>
    To start the service and make sure it is started after every reboot, on each
    node, run:
   </para>
<screen>systemctl enable munge.service
systemctl start munge.service</screen>
    <para>
     To perform the same on multiple nodes, you can also use
     <command>pdsh</command>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-remote-mrsh">
  <title>mrsh/mrlogin &mdash; remote login using MUNGE authentication</title>
  <para>
   <emphasis>mrsh</emphasis> is a set of remote shell programs using the
   <emphasis>MUNGE</emphasis> authentication system instead of reserved ports
   for security.
  </para>
  <para>
   It can be used as a drop-in replacement for <literal>rsh</literal> and
   <literal>rlogin</literal>.
  </para>
  <para>
   To install <emphasis>mrsh</emphasis>, do the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     If only the <emphasis>mrsh</emphasis> client is required (without
     allowing remote login to this machine), use:
     <command>zypper in mrsh</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     To allow logging in to a machine, the server needs to be installed:
     <command>zypper in mrsh-server</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     To get a drop-in replacement for <command>rsh</command> and
     <command>rlogin</command>, run: <command>zypper in
     mrsh-rsh-server-compat</command> or <command>zypper in
     mrsh-rsh-compat</command>.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   To set up a cluster of machines allowing remote login from each other,
   first follow the instructions for setting up and starting MUNGE
   authentication in <xref linkend="sec-remote-munge"/>. After the MUNGE service
   has been successfully started, enable and start <command>mrlogin</command>
   on each machine on which the user will log in:
  </para>
  <screen>systemctl enable mrlogind.socket mrshd.socket
systemctl start mrlogind.socket mrshd.socket</screen>
  <para>
   To start mrsh support at boot, run:
  </para>
  <screen>systemctl enable munge.service
systemctl enable mrlogin.service</screen>
  <para>
   We do not recommend using <emphasis>mrsh</emphasis> when logged in as the
   user <systemitem class="username">root</systemitem>. This is disabled by
   default. To enable it regardless, run:
  </para>
  <screen>echo "mrsh" &gt;&gt; /etc/securetty
  echo "mrlogin" &gt;&gt; /etc/securetty</screen>
 </sect1>
</chapter>
