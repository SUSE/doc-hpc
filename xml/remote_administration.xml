<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-remote" xml:lang="en"
         xmlns="http://docbook.org/ns/docbook" version="5.1"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Remote administration</title>
 <info>
  <abstract>
   <para>
    HPC clusters usually consist of a small set of identical compute
    nodes. Each set of nodes may consist from a mere handful to thousands
    of machines.
    Managing such compute nodes may be challanging. This chaper
    descibes a number of tools which aid this task.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-remote-genders">
  <title>Genders &mdash; static cluster configuration database</title>
  <para>
   Genders is a static cluster configuration database used for
   configuration management. It allows grouping and addressing sets of
   hosts by attributes and is used by a variety of tools. The Genders
   database is a text file which is usually replicated on each node in a
   cluster.
  </para>
  <para>
   Perl, Python, Lua, C, and C++ bindings are supplied with Genders, the
   respective packages provide man pages or other documentation describing
   the APIs.
  </para>
  <sect2 xml:id="sec-genders-db">
    <title>Genders database format</title>
  <para>
    The Genders database in &shpca; is the plain-text file <filename>/etc/genders</filename>.
    It contains a list of nodenames with their attributes. Each line of
    the genders file may have one of the following formats.
  </para>
    <screen>
nodename                attr[=value],attr[=value],...
nodename1,nodename2,... attr[=value],attr[=value],...
nodenames[A-B]          attr[=value],attr[=value],...
    </screen>
  <para>
    The nodename(s) are without their domain and followed by any number of
    spaces or tabs, and then the comma-separated list of attributes. Every
    attribute can optionally have a value.  The substitution string "%n" can
    be used in an attribute value to represent the nodename.  Nodenames can be
    listed on multiple lines, so a node's attributes can be specified on multiple
    lines.  However, no single node may have duplicate attributes.
  </para>
  <para>
    There must be no spaces embedded in the attribute list and there is
    no provision for continuation lines.  Commas and equal characters are special
    and cannot appear in attribute names or their values.  Comments are prefixed
    with the hash character (#) and can appear anywhere in the file.
  </para>
  <para>
    Ranges for nodenames can be specified in the form:
    prefix[n-m,l-k,...], where n &gt; m and l &gt; k, etc., as an alternative
    to explicit lists of nodenames.
  </para>
  </sect2>
  <sect2 xml:id="sec-nodeattr">
    <title>Nodeattr usage</title>
  <para>
    The command line utitlity <literal>nodeaddr</literal> can be used to query data
    in the genders file. When the genders file is replicated on all nodes, a qery
    can be done wihtout network access. It can be called like followed
  </para>
  <screen>
nodeattr [-q | -n | -s] [-r] attr[=val]
  </screen>
  <para>
    where <literal>-q</literal> is the default option and prints a list nodes
    with <literal>attr[=val]</literal> as hostrange.
  </para>
  <para>
    The <literal>-c</literal> options will give a comma separated, the
    <literal>-s</literal> option a space seperated list of nodes with
    <literal>attr[=val]</literal>.
  </para>
  <para>
    If none of the formatting options are specified, nodeattr returns a zero
    value if the local node has the specified attribute, else nonzero.  The
    <literal>-v</literal> option causes any value associated with the attribute
    to go to stdout. If a node name is specified before the attribute, it is
    queried instead of the
    local node.
  </para>
  <para>
    When <literal>nodeattr</literal> is called with the <literal>-l</literal>
    parameter like
  </para>
    <screen>
nodeattr -l [node]
    </screen>
      <para>
        all attributes for a particular node are printed out. If no node paramteter is
        is given, all attributes of the local node are printed out.
      </para>
      <para>
        With the command
       </para>
        <screen>
nodeattr [-f genders] -k
        </screen>
        <para>
          a syntax check of the genders database is performed, where with the switch
          <literal>-f</literal> an alternative datanbase location can be specified.
        </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-remote-pdsh">
  <title>pdsh &mdash; parallel remote shell program</title>
  <para>
   <literal>pdsh</literal> is a parallel remote shell which can be used
   with multiple back-ends for remote connections. It can run a command on
   multiple machines in parallel.
  </para>
  <para>
   To install pdsh, run <command>zypper in pdsh</command>.
  </para>
  <para>
   On &shpca; the back-ends <literal>ssh</literal>,
   <literal>mrsh</literal>, and <literal>exec</literal> are supported. The
   <literal>ssh</literal> back-end is the default. Non-default login methods
   can be used by either setting the <literal>PDSH_RCMD_TYPE</literal>
   environment variable or by using the <literal>-R</literal> command
   argument.
  </para>
  <para>
   When using the <literal>ssh</literal> back-end, it is important that a
   non-interactive (that is, passwordless) login method is used.
  </para>
  <para>
   The <literal>mrsh</literal> back-end requires the
   <literal>mrshd</literal> to be running on the client. The
   <literal>mrsh</literal> back-end does not require the use of reserved
   sockets. Therefore, it does not suffer from port exhaustion when
   executing commands on many machines in parallel. For information about
   setting up the system to use this back-end, see
   <xref linkend="sec-remote-mrsh"/>
  </para>
  <para>
   Remote machines can either be specified on the command line or
   <command>pdsh</command> can use a <filename>machines</filename> file
   (<filename>/etc/pdsh/machines</filename>), dsh (Dancer's shell) style
   groups or netgroups. Also, it can target nodes based on the currently
   running Slurm jobs.
  </para>
  <para>
   The different ways to select target hosts are realized by modules. Some
   of these modules provide identical options to <command>pdsh</command>.
   The module loaded first will win and consume the option. Therefore, it is
   recommended to limit yourself to a single method and specifying this with
   the <literal>-M</literal> option.
  </para>
  <para>
   The <filename>machines</filename> file lists all target hosts one per
   line. The appropriate netgroup can be selected with the
   <literal>-g</literal> command line option.
  </para>
  <para>
   The following host-list plugins for <command>pdsh</command> are supported:
   <literal>machines</literal>, <literal>slurm</literal>,
   <literal>netgroup</literal> and <literal>dshgroup</literal>.
   Each host-list plugin is provided in a separate package. This avoids
   conflicts between command line options for different plugins which
   happen to be identical and helps to keep installations small and free
   of unneeded dependencies. Package dependencies have been set to prevent
   installing plugins with conflicting command options. To install one of
   the plugins, run:
  </para>
  <screen>zypper in pdsh-<replaceable>PLUGIN_NAME</replaceable></screen>
  <para>
   For more information, see the man page <command>pdsh</command>.
  </para>
 </sect1>
 <sect1 xml:id="sec-remote-powerman">
  <title>PowerMan &mdash; centralized power control for clusters</title>
  <para>
   PowerMan allows manipulating remote power control devices (RPC) from a
   central location. It can control:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     local devices connected to a serial port
    </para>
   </listitem>
   <listitem>
    <para>
     RPCs listening on a TCP socket
    </para>
   </listitem>
   <listitem>
    <para>
     RPCs which are accessed through an external program
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The communication to RPCs is controlled by <quote>expect</quote>-like
   scripts. For a
   list of currently supported devices, see the configuration file
   <filename>/etc/powerman/powerman.conf</filename>.
  </para>
  <para>
   To install PowerMan, run <command>zypper in powerman</command>.
  </para>
  <para>
   To configure it, include the appropriate device file for your RPC
   (<filename>/etc/powerman/*.dev</filename>) in
   <filename>/etc/powerman/powerman.conf</filename> and add devices and
   nodes. The device <quote>type</quote> needs to match the
   <quote>specification</quote> name in one
   of the included device files. The list of <quote>plugs</quote> used for
   nodes need to
   match an entry in the <quote>plug name</quote> list.
  </para>
  <para>
   After configuring PowerMan, start its service by:
  </para>
  <screen>systemctl start powerman.service</screen>
  <para>
   To start PowerMan automatically after every boot, run:
  </para>
  <screen>systemctl enable powerman.service</screen>
  <para>
   Optionally, PowerMan can connect to a remote PowerMan instance. To
   enable this, add the option <literal>listen</literal> to
   <filename>/etc/powerman/powerman.conf</filename>.
  </para>
  <important>
   <title>Unencrypted transfer</title>
   <para>
    When connecting to a remote PowerMan instance, data is transferred
    unencrypted. Therefore, use this feature only if the network is
    appropriately secured.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="sec-remote-munge">
  <title>MUNGE authentication</title>
  <para>
   <emphasis>munge</emphasis> allows for a secure communication between
   different machines which share the same secret key. The most common
   usecase is the <emphasis>slurm</emphasis> workload mangager which
   uses <emphasis>munge</emphasis> for the encryption of its messages.
   Another usecase is authentification for the parallel shell
   <emphasis>mrsh</emphasis>.
  </para>
  <sect2>
   <title>Setting up MUNGE authentication</title>
   <para>
    <emphasis>Munge</emphasis> uses the UID/GID to uniquely identify
    and authenciate users. This care must be taken that the users who
    are to be authenicated across a network have unified UIDs and GIDs.
   </para>
   <para>
    <emphasis>Munge</emphasis> credentials have a limited time-to-live.
    Therefore it must be ensured  that the time is synchronized across
    the entire cluster.
   </para>
   <para>
    <emphasis>Munge</emphasis> is installed by <command>zypper</command>
    in munge'. This will install all packages required at runtime. A
    'munge-devel' package is available to build applications that require
    munge authentication.
   </para>
   <para>
    When installing <emphasis>munge</emphasis> a new key is generated on
    every system. The <emphasis>munge</emphasis> key needs to be uniform
    across the entire cluster, however. Therefore a key of one system
    needs to be copied in a secure way to all other nodes in the cluster.
    Care must be taken that the key is only readable by the
    <emphasis>'munge'</emphasis> user (permissions mask
    <literal>0400</literal>).
   </para>
   <para>
    <emphasis>pdsh</emphasis> (with ssh) may be used to do this:
   </para>
   <para>
    Check permissions, owner and file type of the key file located under
    <filename>/etc/munge/munge.key</filename> on the local system:
   </para>
   <screen># stat --format "%F %a %G %U %n" /etc/munge/munge.key</screen>
   <para>The settings should be:</para>
   <screen>400 regular file munge munge /etc/munge/munge.key</screen>
   <para>Get md5sum of munge.key:</para>
   <screen># md5sum /etc/munge/munge.key</screen>
   <para>Copy key to list of nodes:</para>
   <screen># pdcp -R ssh -w &lt;hostlist&gt; /etc/munge/munge.key
   /etc/munge/munge.key </screen>
   <para>Check key settings in remote host:</para>
   <screen>pdsh -R ssh -w &lt;hostlist&gt; stat --format \"%F %a %G %U %n\"
   /etc/munge/munge.key
   pdsh -R ssh -w &lt;hostlist&gt; md5sum /etc/munge/munge.key
   </screen>
   <para>make sure they match.</para>
  </sect2>
  <sect2>
   <title>Enabling and starting MUNGE</title>
   <para>
    <emphasis>munged</emphasis> needs to be run on all nodes where munge
    authentication is to take place. If munge is used for authentication
    across the network it needs to run on each side of the communication.
   </para>
   <para>
    To start the service and make sure it is started after every reboot, on each
    node, run:
   </para>
<screen>systemctl enable munge.service
systemctl start munge.service</screen>
    <para>
     To perform the same on multiple nodes, you can also use
     <command>pdsh<command>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-remote-mrsh">
  <title>mrsh/mrlogin &mdash; remote login using MUNGE authentication</title>
  <para>
   <emphasis>mrsh</emphasis> is a set of remote shell programs using the
   <emphasis>munge</emphasis> authentication system instead of reserved ports
   for security.
  </para>
  <para>
   It can be used as a drop-in replacement for <literal>rsh</literal> and
   <literal>rlogin</literal>.
  </para>
  <para>
   To install <emphasis>mrsh</emphasis>, do the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     If only the <emphasis>mrsh</emphasis> client is required (without
     allowing remote login to this machine), use:
     <command>zypper in mrsh</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     To allow logging in to a machine, the server needs to be installed:
     <command>zypper in mrsh-server</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     To get a drop-in replacement for <command>rsh</command> and
     <command>rlogin</command>, run: <command>zypper in
     mrsh-rsh-server-compat</command> or <command>zypper in
     mrsh-rsh-compat</command>.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   To set up a cluster of machines allowing remote login from each other,
   first follow the instructions for setting up and starting
   <emphasis>munge</emphasis> authentication in <xref linkend="sec-remote-munge"/>.
   After <emphasis>munge</emphasis> service has been successfully
   started, enable and start <command>mrlogin</command> on each machine on
   which the user will log in:
  </para>
  <screen>systemctl enable mrlogind.socket mrshd.socket
systemctl start mrlogind.socket mrshd.socket</screen>
  <para>
   To start mrsh support at boot, run:
  </para>
  <screen>systemctl enable munge.service
systemctl enable mrlogin.service</screen>
  <para>
   We do not recommend using <emphasis>mrsh</emphasis> when logged in as the
   user <systemitem class="username">root</systemitem>. This is disabled by
   default. To enable it regardless, run:
  </para>
  <screen>echo "mrsh" &gt;&gt; /etc/securetty
  echo "mrlogin" &gt;&gt; /etc/securetty</screen>
 </sect1>
</chapter>
