<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-monitoring" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Monitoring and logging</title>
 <info>
  <abstract>
   <para>
    Obtaining and maintaining an overview over the status and health
    of a compute cluster nodes helps to ensure a smooth operation.
    A number of tools are provided which give an administrator an
    overview over the current cluster status, collect system
    logs and gather information on certain system failures conditions.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
  <sect1 xml:id="sec-remote-conman">
  <title>ConMan &mdash; the console manager</title>
  <para>
   ConMan is a serial console management program designed to support a
   large number of console devices and simultaneous users. It supports:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     local serial devices
    </para>
   </listitem>
   <listitem>
    <para>
     remote terminal servers (via the telnet protocol)
    </para>
   </listitem>
   <listitem>
    <para>
     IPMI Serial-Over-LAN (via FreeIPMI)
    </para>
   </listitem>
   <listitem>
    <para>
     Unix domain sockets
    </para>
   </listitem>
   <listitem>
    <para>
     external processes (for example, using <command>expect</command> scripts
     for <command>telnet</command>, <command>ssh</command>, or
     <command>ipmi-sol</command> connections)
    </para>
   </listitem>
  </itemizedlist>
  <para>
   ConMan can be used for monitoring, logging and optionally timestamping
   console device output.
  </para>
  <para>
   To install ConMan, run <literal>zypper in conman</literal>.
  </para>
  <important>
   <title><systemitem class="daemon">conmand</systemitem> sends unencrypted data</title>
   <para>
    The daemon <systemitem class="daemon">conmand</systemitem> sends
    unencrypted data over the
    network and its connections are not authenticated. Therefore, it should
    be used locally only: Listening to the port
    <literal>localhost</literal>. However, the IPMI console does offer
    encryption. This makes <literal>conman</literal> a good tool for
    monitoring a large number of such consoles.
   </para>
  </important>
  <para>
   Usage:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     ConMan comes with a number of expect-scripts. They can be found in the
     directory <filename>/usr/lib/conman/exec</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Input to <literal>conman</literal> is not echoed in interactive mode.
     This can be changed by entering the escape sequence
     <literal>&amp;E</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     When pressing <keycap function="enter"/> in interactive mode, no line
     feed is generated. To generate a line feed, press
     <keycombo><keycap function="control"/><keycap>L</keycap></keycombo>.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   For more information about options, see the man page of ConMan.
  </para>
 </sect1>
 <sect1 xml:id="sec-monitoring-clusters-prometheus">
  <title>Monitoring &hpca; clusters</title>
  <para>
   TODO: Add intro
  </para>
  <para>
   In the following procedure, the cluster contains a head node (&wsIname;), a
   monitoring server (&wsIIname;), and two compute nodes (&wsIIIname; and &wsIVname;).
  </para>
  <procedure xml:id="pro-monitoring-clusters-prometheus">
   <title>Monitoring &hpca; clusters with &prometheus; and &grafana;</title>
   <step>
    <para>
     On the head node, install the &slurm; exporter:
    </para>
<screen>&prompt.root;zypper in golang-github-vpenso-prometheus_slurm_exporter
&prompt.root;systemctl enable --now prometheus-slurm_exporter</screen>
    <para>
     Verify that the &slurm; exporter is working by navigating to
     http://&wsIname;:8080/metrics in your browser, where you should see the
     exported metrics to be collected by &prometheus;.
    </para>
    <remark>trichardson (2021-07-08): Cannot connect in my test lab, so unsure
     if the Slurm exporter works</remark>
   </step>
   <step>
    <para>
     On each compute node, install the node exporter:
    </para>
<screen>&prompt.root;zypper in golang-github-prometheus-node_exporter
&prompt.root;systemctl enable --now prometheus-node_exporter</screen>
    <para>
     Verify that the node exporter is working by navigating to
     http://&wsIname;:9100/metrics in your browser, where you should see the
     exported metrics to be collected by &prometheus;.
    </para>
    <remark>trichardson (2021-07-08): I can connect to <emphasis>this</emphasis>
     address in my test lab, which suggests that the problem with 8080 is the
     Slurm exporter rather than my test lab</remark>
   </step>
   <step>
    <para>
     On the monitoring server, install &prometheus; and &grafana;:
    </para>
    <substeps>
     <step>
      <para>
       Install PackageHub:
      </para>
<screen>&prompt.root;SUSEConnect --product PackageHub/15.3/x86_64</screen>
     </step>
     <step>
      <para>
       Install the &prometheus; and &grafana; packages:
      </para>
<screen>&prompt.root;zypper in golang-github-prometheus-prometheus grafana</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Edit the <literal>scrape_configs</literal> section of the
     <filename>/etc/prometheus/prometheus.yml</filename> file to add jobs
     for each node:
    </para>
<screen>scrape_configs:

  - job_name: 'monitoring-server'
    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s
    scrape_timeout: 5s
    static_configs:
      - targets: ['localhost:9090']
      - targets: ['localhost:9100']

  - job_name: 'head-node'
    static_configs:
      - targets: ['&wsIname;:8080']
      - targets: ['&wsIname;:9100']

  - job_name: 'node1'
    static_configs:
      - targets: ['&wsIIIname;:9100']

  - job_name: 'node2'
    static_configs:
      - targets: ['&wsIVname;:9100']</screen>
    <remark>trichardson (2021-07-09): Do you need to add a job for every compute
     node? What if you have hundreds?</remark>
   </step>
<!--
The first job ‘prometheus_server’ is used to monitor the Prometheus server itself, and the second job ‘slurmctld’ is for the SLURM exporter metrics.
+
In both jobs, a new line has been added for the node exporter data served from port 9100.
-->
   <step>
    <para>
     Enable and start &prometheus; and &grafana;:
    </para>
<screen>&prompt.root;systemctl enable --now prometheus
&prompt.root;systemctl enable --now grafana-server</screen>
    <para>
     Verify that &prometheus; is working by navigating to
     http://&wsIIname;:9090/config in your browser, where you should see the
     contents of the <filename>/etc/prometheus/prometheus.yml</filename> file.
    </para>
   </step>
   <step>
    <para>
     Log in to the &grafana; web interface at http://&wsIIname;:3000 using
     <literal>admin</literal> for both the user name and password, then change
     your password when prompted.
    </para>
   </step>
   <step>
    <para>
     Add &prometheus; as a data source:
    </para>
    <substeps>
     <step>
      <para>
       Navigate to <menuchoice><guimenu>Configuration</guimenu><guimenu>Data Sources</guimenu></menuchoice>,
       then click <guimenu>Add data source</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Find &prometheus; and click <guimenu>Select</guimenu>.
      </para>
     </step>
     <step>
      <para>
       In the <guimenu>URL</guimenu> field, enter <literal>http://localhost:9090</literal>.
       You can keep the default settings for the other fields.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Save &amp; Test</guimenu>.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Add a dashboard for the &slurm; metrics:
    </para>
    <substeps>
     <step>
      <para>
       Navigate to <menuchoice><guimenu>Create</guimenu><guimenu>Import</guimenu></menuchoice>.
      </para>
     </step>
     <step>
      <para>
       In the <guimenu>Import via grafana.com</guimenu> field, enter the dashboard
       ID <literal>4323</literal>, then click <guimenu>Load</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Select <guimenu>&prometheus;</guimenu> from the
       <guimenu>Select a &prometheus; data source</guimenu> drop down box, then
       click <guimenu>Import</guimenu>.
      </para>
      <!-- you'll now see the slurm dashboard -->
     </step>
     <step>
      <para>
       Click the <guimenu>Save</guimenu> icon, optionally add a note to describe
       your changes, then click <guimenu>Save</guimenu>.
      </para>
     </step>
    </substeps>
    <remark>trichardson (2021-07-09): No data, unsurprising since I couldn't
     access the slurm exporter web server</remark>
   </step>
   <step>
    <para>
     Add a dashboard for the node metrics:
    </para>
    <substeps>
     <step>
      <para>
       Navigate to <menuchoice><guimenu>Create</guimenu><guimenu>Import</guimenu></menuchoice>.
      </para>
     </step>
     <step>
      <para>
       In the <guimenu>Import via grafana.com</guimenu> field, enter the dashboard
       ID <literal>405</literal>, then click <guimenu>Load</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Select <guimenu>&prometheus;</guimenu> from the
       <guimenu>Select a &prometheus; data source</guimenu> drop down box, then
       click <guimenu>Import</guimenu>.
      </para>
      <!-- you'll now see the nodes dashboard -->
     </step>
     <step>
      <para>
       Click the <guimenu>Save</guimenu> icon, optionally add a note to describe
       your changes, then click <guimenu>Save</guimenu>.
      </para>
     </step>
    </substeps>
    <remark>trichardson (2021-07-09): It took a very long time for data from the nodes to appear. Is this normal? If so, it may be worth mentioning, otherwise people could think it didn't work.</remark>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-monitoring-ganglia">
  <title>Ganglia &mdash; system monitoring</title>
  <para>
   Ganglia is a scalable distributed monitoring system for high-performance
   computing systems, such as clusters and grids. It is based on a
   hierarchical design targeted at federations of clusters.
  </para>
  <sect2 xml:id="sec-using-ganglia">
   <title>Using Ganglia</title>
   <para>
    To use Ganglia, make sure to install <package>ganglia-gmetad</package>
    on the management server, then start the Ganglia meta-daemon:
    <command>rcgmead start</command>. To make sure the service is started
    after a reboot, run: <command>systemctl enable gmetad</command>. On
    each cluster node which you want to monitor, install
    <package>ganglia-gmond</package>, start the service <command>rcgmond
    start</command> and make sure it is enabled to be started automatically
    after a reboot: <command>systemctl enable gmond</command>. To test
    whether the <systemitem class="daemon">gmond</systemitem> daemon has
    connected to the
    meta-daemon, run <command>gstat -a</command> and check that each node to
    be monitored is present in the output.
   </para>
  </sect2>
  <sect2 xml:id="sec-ganglia-btrfs">
   <title>Ganglia on Btrfs</title>
   <para>
    When using the Btrfs file system, the monitoring data will be lost after
    a rollback and the service <systemitem class="daemon">gmetad</systemitem>.
    To be fix this issue, either install the package
    <package>ganglia-gmetad-skip-bcheck</package> or create the file
    <filename>/etc/ganglia/no_btrfs_check</filename>.
   </para>
  </sect2>
  <sect2 xml:id="sec-ganglia-web">
   <title>Using the Ganglia Web interface</title>
   <para>
    Install <package>ganglia-web</package> on the management server.
    Depending on which PHP version is used (the default is PHP 7), enable it in
    Apache2: <command>a2enmod php7</command> or <command>a2enmod
    php7</command>. Then start Apache2 on this machine: <command>rcapache2
    start</command> and make sure it is started automatically after a
    reboot: <command>systemctl enable apache2</command>. The Ganglia Web
    interface should be accessible from
    <literal>http://<replaceable>MANAGEMENT_SERVER</replaceable>/ganglia</literal>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-monitoring-ras">
  <title>rasdaemon &mdash; utility to log RAS error tracings</title>
  <para>
   <systemitem class="daemon">rasdaemon</systemitem> is a RAS
   (Reliability, Availability and Serviceability) logging tool. It records
   memory errors using EDAC (Error Detection and Correction) tracing events.
   EDAC drivers in the Linux kernel handle detection of ECC (Error Correction
   Code) errors from memory controllers.
  </para>
  <para>
   <systemitem class="daemon">rasdaemon</systemitem> can be used on large
   memory systems to track, record and localize memory errors and how they
   evolve over time to detect hardware degradation. Furthermore, it can be used
   to localize a faulty DIMM on the motherboard.
  </para>
  <para>
   To check whether the EDAC drivers are loaded, execute:
  </para>
<screen>&prompt;ras-mc-ctl --status</screen>
  <para>
   The command should return <literal>ras-mc-ctl: drivers are
   loaded</literal>. If it indicates that the drivers are not loaded, EDAC
   may not be supported on your board.
  </para>
  <para>
   To start <systemitem class="daemon">rasdaemon</systemitem>, run
   <command>systemctl start rasdaemon.service</command>.
   To start <systemitem class="daemon">rasdaemon</systemitem>
   automatically at boot time, execute <command>systemctl enable
   rasdaemon.service</command>. The daemon will log information to
   <filename>/var/log/messages</filename> and to an internal database. A
   summary of the stored errors can be obtained with:
  </para>
<screen>&prompt;ras-mc-ctl --summary</screen>
  <para>
   The errors stored in the database can be viewed with:
  </para>
<screen>&prompt;ras-mc-ctl --errors</screen>
  <para>
   Optionally, you can load the DIMM labels silk-screened on the system
   board to more easily identify the faulty DIMM. To do so, before starting
   <systemitem class="daemon">rasdaemon</systemitem>, run:
  </para>
<screen>&prompt;systemctl start ras-mc-ctl start</screen>
  <para>
   For this to work, you need to set up a layout description for the board.
   There are no descriptions supplied by default. To add a layout
   description, create a file with an arbitrary name in the directory
   <filename>/etc/ras/dimm_labels.d/</filename>. The format is:
  </para>
<screen>Vendor: <replaceable>MOTHERBOARD-VENDOR-NAME</replaceable>
Model: <replaceable>MOTHERBOARD-MODEL-NAME</replaceable>
  <replaceable>LABEL</replaceable>: <replaceable>MC</replaceable>.<replaceable>TOP</replaceable>.<replaceable>MID</replaceable>.<replaceable>LOW</replaceable></screen>
 </sect1>
</chapter>
