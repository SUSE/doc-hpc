<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-scheduler" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Scheduler</title>
 <info>
  <abstract>
   <para>
    What is this chapter about?
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-scheduler-slurm">
   <title>Slurm &mdash; Utility for HPC Workload Management</title>
   <para>
    Slurm is an open-source, fault-tolerant, and highly scalable cluster
    management and job scheduling system for Linux clusters containing up to
    65,536 nodes. Components include machine status, partition management,
    job management, scheduling and accounting modules.
   </para>
   <para>
    For a minimal setup to run Slurm with <emphasis>munge</emphasis> support on
    one compute
    node and multiple control nodes, follow these instructions:
   </para>
   <procedure>
    <step>
     <para>
      Before installing Slurm, create a user and a group called
      <literal>slurm</literal>.
     </para>
     <important>
      <title>Make Sure of Consistent UIDs and GIDs for Slurm's Accounts</title>
      <para>
       For security reasons, Slurm does not run as the user
       <systemitem class="username">root</systemitem> but under its own
       user. It is important that the user
       <systemitem class="username">slurm</systemitem> has the
       same UID/GID across all nodes of the cluster.
      </para>
      <para>
       If this user/group does not exist, the package
       <package>slurm</package> creates this user and group when it is
       installed. However, this does not guarantee
       that the generated UIDs/GIDs will be identical on all systems.
      </para>
      <para>
       Therefore, we strongly advise you to create the user/group
       <systemitem class="username">slurm</systemitem> before
       installing <package>slurm</package>.
       If you are using a network directory service such as LDAP for user and
       group management, you can use it to
       provide the <systemitem class="username">slurm</systemitem>
       user/group as well.
      </para>
     </important>
    </step>
    <step>
     <para>
      Install <package>slurm-munge</package> on the control and compute
      nodes: <command>zypper in slurm-munge</command>
     </para>
    </step>
    <step>
     <para>
      Configure, enable and start "munge" on the control and compute nodes.
      as described in <xref linkend="sec-remote-munge"/>.
     </para>
    </step>
    <step>
     <para>
      On the compute node, edit <filename>/etc/slurm/slurm.conf</filename>:
     </para>
     <substeps>
      <step>
       <para>
        Configure the parameter
        <literal>ControlMachine=<replaceable>CONTROL_MACHINE</replaceable></literal>
        with the host name of the control node.
       </para>
       <para>
        To find out the correct host name, run
        <command>hostname -s</command> on the control node.
       </para>
      </step>
      <step>
       <para>
        Additionally add:
       </para>
<screen>NodeName=<replaceable>NODE_LIST</replaceable> Sockets=<replaceable>SOCKETS</replaceable> \
  CoresPerSocket=<replaceable>CORES_PER_SOCKET</replaceable> \
  ThreadsPerCore=<replaceable>THREADS_PER_CORE</replaceable> \
  State=UNKNOWN</screen>
       <para>
        and
       </para>
<screen>PartitionName=normal Nodes=<replaceable>NODE_LIST</replaceable> \
  Default=YES MaxTime=24:00:00 State=UP</screen>
       <para>
        where <replaceable>NODE_LIST</replaceable> is the list of compute
        nodes (that is, the output of <command>hostname -s</command> run on
        each compute node (either comma-separated or as ranges:
        <literal>foo[1-100]</literal>). Additionally,
        <replaceable>SOCKETS</replaceable> denotes the number of sockets,
        <replaceable>CORES_PER_SOCKET</replaceable> the number of cores per
        socket, <replaceable>THREADS_PER_CORE</replaceable> the number of
        threads for CPUs which can execute more than one thread at a time.
        (Make sure that <replaceable>SOCKETS</replaceable> *
        <replaceable>CORES_PER_SOCKET</replaceable> *
        <replaceable>THREADS_PER_CORE</replaceable> does not exceed the
        number of system cores on the compute node).
       </para>
      </step>
      <step>
       <para>
        On the control node, copy <filename>/etc/slurm/slurm.conf</filename>
        to all compute nodes:
       </para>
<screen>scp /etc/slurm/slurm.conf <replaceable>COMPUTE_NODE</replaceable>:/etc/slurm/</screen>
      </step>
      <step>
       <para>
        On the control node, start <systemitem class="daemon">slurmctld</systemitem>:
       </para>
<screen>systemctl start slurmctld.service</screen>
       <para>
        Also enable it so that it starts on every boot:
       </para>
<screen>systemctl enable slurmctld.service</screen>
      </step>
      <step>
       <para>
        On the compute nodes, start and enable
        <systemitem class="daemon">slurmd</systemitem>:
       </para>
<screen>systemctl start slurmd.service
systemctl enable slurmd.service</screen>
       <para>
        The last line causes <systemitem class="daemon">slurmd</systemitem>
        to be started on
        every boot automatically.
       </para>
      </step>
     </substeps>
    </step>
   </procedure>
   <note>
    <title>Epilog Script</title>
    <para>
     The standard epilog script will kill all remaining processes of a user
     on a node. If this behavior is not wanted, disable the standard epilog
     script.
    </para>
   </note>
   <para>
    For further documentation, see the
    <link xlink:href="https://slurm.schedmd.com/quickstart_admin.html">Quick Start
        Administrator Guide</link> and
<link xlink:href="https://slurm.schedmd.com/quickstart.html"> Quick Start User
    Guide</link>. There is further in-depth documentation on the
<link xlink:href="https://slurm.schedmd.com/documentation.html">Slurm
    documentation page</link>.
   </para>
 </sect1>
 <!--
 <sect1 xml:id="sec-scheduler-conf">
  <title>Configuration</title>
  <para>
   TOFIX
  </para>
  <orderedlist>
   <listitem>
    <para>
     An
    </para>
   </listitem>
   <listitem>
    <para>
     Ordered
    </para>
   </listitem>
   <listitem>
    <para>
     List
    </para>
   </listitem>
  </orderedlist>
 </sect1>
  -->
</chapter>
